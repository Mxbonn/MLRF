guid,question,answer,paper_title,paper_url,explanation,tags
OMqjUp6Xq8,What is <b>The Lottery Ticket Hypothesis</b>?,"A randomly-initialized, dense neural network contains a subnetwork that is initialized such that - when trained in isolation - it can match the test accuracy of the&nbsp;original network after training for at most the same number of iterations.","The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",https://arxiv.org/abs/1803.03635,"More formally, consider a dense feed-forward neural network \(f(x; \theta)\) with initial parameters \(\theta = \theta_0 âˆ¼ \mathcal{D}_0\). When optimizing with stochastic gradient descent (SGD) on a training set, \(f\) reaches minimum validation loss \(l\) at iteration \(j\) with test accuracy \(a\) . In addition, consider training \(f(x;m\bigodot\theta)\) with a mask \(m \in {0, 1}^{|\theta|}\) on its parameters such that its initialization is \(m \bigodot \theta_0\). When optimizing with SGD on the same training set (with \(m\) fixed), \(f\) reaches minimum validation loss \(l'\) at iteration \(j'\) with test accuracy \(a'\). The lottery ticket hypothesis predicts that \(\exists m \) for which \(j' \le j\) (commensurate&nbsp;training time), \(a' \ge a\) (commensurate accuracy), and \(||m|| \ll |\theta|\) (fewer parameters).",
Nlq3^%1eiC,What does the <b>Lottery Ticket Hypothesis</b>&nbsp;say about the initialization of the <i>winning tickets</i>?,"When the parameters of the <i>winning tickets</i>&nbsp;are <u>randomly reinitialized</u>&nbsp;\((f(x;m\bigodot\theta'_0)\), these <i>winning tickets</i>&nbsp;<b>no longer match the performance of the original network</b>, offering evidence that these smaller networks do not train effectively unless they are appropriately initialized.","The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",https://arxiv.org/abs/1803.03635,"This means that the network structure alone cannot explain the winning ticket's success.<br>(However at moderate pruning levels and in conv-nets, the structure alone may be sufficient)",
eke9XXq_VX,How is the <b>pruning</b> done in the <b>Lottery Ticket Hypothesis paper</b>?,<b>Iterative</b>&nbsp;pruning with resetting the remaining parameters to their initial values (so also retraining from scratch after every pruning step).,"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",https://arxiv.org/abs/1803.03635,,
u0)t(9J(F9,What is the <b>importance of the winning ticket initialization</b>?,"<b>When randomly reinitialized, a winning ticket learns more slowly and achieves lower test accuracy</b>, suggesting that initialization is important to&nbsp;its success.<br><br>Experiments have shown that the winning ticket weights move further than other weights. This suggests that <b>the benefit of the initialization is connected to the optimization algorithm, dataset,&nbsp;and model.&nbsp;</b>For example, the winning ticket initialization might land in a region of the loss landscape that is particularly amenable to optimization by the chosen optimization algorithm.","The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",https://arxiv.org/abs/1803.03635,"The authors hypothesize that up to a certain level of sparsity - highly overparameterized networks can be pruned, reinitialized, and retrained successfully; however, beyond this point, extremely pruned, less severely overparamterized&nbsp;networks only maintain accuracy with fortuitous initialization.",
kC)FlP8JZ&,What were some important limitations of the <b>Lottery Ticket Hypothesis</b>&nbsp;paper?,"They only considered vision-centric classification tasks on smaller datasets (MNIST, CIFAR10). Iterative pruning is computationally intensive, requiring training a network 15 or more times consecutively for multiple&nbsp;trials. This pruning flow is also the only method to find winning tickets.","The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",https://arxiv.org/abs/1803.03635,,
