guid,question,answer,paper_title,paper_url,explanation,tags
oNYcS4=MO*,Give <b>schematic</b> of the contrastive learning framework used in <b>SimCLR</b>.,"<img src=""SimCLR.png""><br>",A Simple Framework for Contrastive Learning of Visual Representations,https://arxiv.org/abs/2002.05709,"Framework for contrastive learning of visual representations.<br>Two separate data augmentations operators are sampled from the same family of augmentations (\(t, t' \sim \mathcal{T}\)) and applied to each data example to obtain two correlated views. A base encoder network \(f(.)\) and a projection head \(g(.)\) are trained to maximize agreement using a contrastive loss. After training is completed, throw away the projection head \(g(.)\) and use encoder \(f(.)\) and representation \(\mathbf{h}\) for downstream tasks.",
ra_/Ps?d3},Which <b>similarity metric</b> is used in <b>SimCLR</b>?,"<b>Cosine similarity<br></b>This can be represented by using a dot product and scaling by the magnitudes.<br>\[s(\mathbf{u}, \mathbf{v}) = \frac{\mathbf{u}^T\mathbf{v}}{\|u\| \|v\|}\]<br>",A Simple Framework for Contrastive Learning of Visual Representations,https://arxiv.org/abs/2002.05709,,
NVam[+?`DK,Which <b>loss</b> function is used in <b>SimCLR</b>?,"The<b> loss function</b> for a <b>positive pair</b> of examples \((i, j)\) is defined as:<br>\[\begin{aligned}
\mathcal{L}_\text{SimCLR}^{(i,j)} &amp;= - \log\frac{\exp(s(\mathbf{z}_i, \mathbf{z}_j) / \tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k \neq i]} \exp(s(\mathbf{z}_i, \mathbf{z}_k) / \tau)}
\end{aligned}\]where \(s(.)\) is the similarity metric (usually cosine similarity).<br>The <b>final loss is computed across all positive pairs</b>, both \((i,j)\) and \((j,i)\).",A Simple Framework for Contrastive Learning of Visual Representations,https://arxiv.org/abs/2002.05709,This loss can be called the<b> normalized temperature-scaled cross entropy loss</b> (NT-Xent).<br>It has been used in prior work.,
O{WoQ5y+Zs,Give the <b>training algorithm</b> for <b>SimCLR</b>.,"<div style=""text-align: left;""><b>input</b>: batch size \(N\), temperature constant \(\tau\), encoder \(f\), projection head \(g\), augmentation family \(\mathcal{T}\).<br>for sampled minibatch \(\{\mathbf{x}_k\}^N_{k=1}\) do:<br>&nbsp; &nbsp; for all \(k \in \{1, \dots, N\}\) do:<br>&nbsp; &nbsp; &nbsp; &nbsp; sample two augmentation functions \(t \sim \mathcal{T}\), \(t' \sim \mathcal{T}\)<br>&nbsp; &nbsp; &nbsp; &nbsp; \(\tilde{\mathbf{x}}_{2k - 1}= t(\mathbf{x}_k)\)<br>&nbsp; &nbsp; &nbsp; &nbsp; \(\tilde{\mathbf{x}}_{2k}= t'(\mathbf{x}_k)\)<br>&nbsp; &nbsp; &nbsp; &nbsp; \(\mathbf{h}_{2k - 1}= f(\tilde{\mathbf{x}}_{2k -1 })\)<br>&nbsp; &nbsp; &nbsp; &nbsp; \(\mathbf{h}_{2k}= f(\tilde{\mathbf{x}}_{2k})\)<br>&nbsp; &nbsp; &nbsp; &nbsp; \(\mathbf{z}_{2k-1} = g(\mathbf{h}_{2k-1})\)<br>&nbsp; &nbsp; &nbsp; &nbsp; \(\mathbf{z}_{2k} = g(\mathbf{h}_{2k})\)<br>&nbsp; &nbsp; for all \(i \in \{1, \dots, 2N\}\) and \(j \in \{1, \dots, 2N\}\) do:<br>&nbsp; &nbsp; &nbsp; &nbsp; \(s_{i,j} = \frac{\mathbf{z}_i^\top\mathbf{z}_j}{\|\mathbf{z}_i\| \|\mathbf{z}_j\|}\)<br>&nbsp; &nbsp; define \(\mathcal{L}^{(i,j)} = - \log\frac{\exp(s_{i,j} / \tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k \neq i]} \exp(s_{i,k} / \tau)}\)<br>&nbsp; &nbsp; \(\mathcal{L} = \frac{1}{2N} \sum^N_{k=1}[\mathcal{L}^{(2k-1,2k)} +\mathcal{L}^{(2k,2k-1)}]\)<br>&nbsp; &nbsp; update networks \(f\) and \(g\) to minimize \(\mathcal{L}\)<br><b>return</b> encoder \(f\) and throw away \(g\)<br></div>",A Simple Framework for Contrastive Learning of Visual Representations,https://arxiv.org/abs/2002.05709,,
r%|IZ/Sog),"In contrastive frameworks such as SimCLR, <b>why is the similarity optimized on a separate projection head</b> \(g\)?","It likely due to the fact that the contrastive representation needs to be invariant to many data transformations, as such information such as color is removed in this representation while this may be useful for downstream tasks. By adding an additional projection head, \(g\) can remove information that may be useful for downstream tasks but needs to be removed in order to maximize the contrastive similarity.However all of this is found empirically.",A Simple Framework for Contrastive Learning of Visual Representations,https://arxiv.org/abs/2002.05709,,
