guid,question,answer,paper_title,paper_url,explanation,tags
41lNu-X}|,Give the algorithm for&nbsp;<b>Token Merging</b>.,<ol><li>Partition the tokens into two sets \(\mathbb{A}\)&nbsp;and \(\mathbb{B}\) by alternating the assignment.</li><li>Calculate the similarity score for all tokens in \(\mathbb{A}\) by taking the cosine similarity of the \(\mathbf{K}\) vector of a token in \(\mathbb{A}\) to all tokens in \(\mathbb{B}\). The final similarity score of a token in \(\mathbb{A}\) is the highest pairwise score.</li><li>Merge the \(k\) pairs with the highest similarity score</li><li>Concatenate the two sets back together</li></ol>,Token Merging: Your ViT But Faster,https://arxiv.org/abs/2210.09461,"<img src=""paste-e674aa12e8c0e8302daa5e55a32754f3c9978627.jpg"">",
CLLGe@Z<i9,"Apart from the merging module, which other change needs to be made to the standard Vision Transformer in order to apply&nbsp;<b>Token Merging</b>?","The standard attention function need to be changed to&nbsp;<b>proportional attention</b>:<br>\[\mathbf{A} = \operatorname{softmax}(\frac{\mathbf{QK}^T}{\sqrt{d}} + \log(\mathbf{s}))\]<i>where \(\mathbf{s}\)&nbsp;is a row vector containing the&nbsp;size&nbsp;of each token (number of patches the token represents)</i>. Tokens are also weighted by \(\mathbf{s}\) any time they are aggregated, like when the tokens are merged together.",Token Merging: Your ViT But Faster,https://arxiv.org/abs/2210.09461,This performs the same operation as if you'd have&nbsp;<i>s</i>&nbsp;copies of the key,
MGVm;!!J{A,Where is the&nbsp;<b>Token Merging</b>&nbsp;module inserted in the vision transformer?,Between the Multi-headed Self-Attention (MSA) layer and the MLP layer.,Token Merging: Your ViT But Faster,https://arxiv.org/abs/2210.09461,,
j+k)D&~x:},What schedule does&nbsp;<b>Token Merging</b>&nbsp;use to merge tokens?,The default setting merges a <b>fixed </b>\(k\) tokens per layer.&nbsp;<br>They also report on a linearly decreasing schedule.,Token Merging: Your ViT But Faster,https://arxiv.org/abs/2210.09461,,
