guid,question,answer,paper_title,paper_url,explanation,tags
stLI37-h^E,"<b>Which base task does <u>DUSt3R</u> perform</b> from which it can directly solve downstream tasks such as camera pose estimation, depth estimation, 3D reconstruction, etc..",<b>D</b>ense and <b>U</b>nconstrained <b>St</b>ereo <b>3</b>D <b>R</b>econstruction of arbitrary image collections.<br>Given a pair of images they regress the <i>pointmaps</i>. Where a <i>pointmap</i>&nbsp;is the a dense 2D field of 3D points associated with its corresponding RGB image.,DUSt3R: Geometric 3D Vision Made Easy,https://arxiv.org/abs/2312.14132,"<img src=""paste-4c84bd7800a38e28425fc5d64999659a5f97db04.jpg"">",
Ez8mQ&IJ*v,Give an overview of the <b>DUSt3R</b> architecture.,"<img alt=""Refer to caption"" src=""x2.png""><br>Two views of a scene \((I^1, I^2)\) are first encoded in a Siamese manner with a <b>shared ViT encoder</b>. The resulting token representations&nbsp;\(F^1\)&nbsp;and \(F^2\)&nbsp;are then passed to <b>two transformer decoders</b> that constantly exchange information <b>via cross-attention</b>.<br>Finally, <b>two regression heads output the two corresponding pointmaps</b> and associated <b>confidence maps</b>.<br>Importantly, the two pointmaps are expressed in the same coordinate frame of the first image \(I^1\).",DUSt3R: Geometric 3D Vision Made Easy,https://arxiv.org/abs/2312.14132,"The network architecture is inspired by&nbsp;<a href=""https://arxiv.org/abs/2210.10716"">CroCo: Self-Supervised Pre-training for 3D Vision Tasks by Cross-View Completion</a>",
E8Dc$S<-g,<b>What are the inputs and outputs of the <u>DUSt3R</u> network</b> and what data do you need to setup this input/output?,"The <b>input</b> is two input RGB images that correspond to two views of a scene: \(I^1, I^2 \in \mathbb{R}^{W\times H \times 3}\)<br>The <b>outputs</b> are the 2 corresponding <i>pointmaps</i>, expressed in the coordinate frame of \(I^1\): \(X^{1,1}, X^{2,1}\in \mathbb{R}^{W\times H \times 3}\) with associated confidence maps&nbsp;&nbsp;\(C^{1,1}, C^{2,1}\in \mathbb{R}^{W\times H \times 3}\).<br><br>To <b>construct these outputs</b>, you need to know the camera <b>intrinsics</b> \(K \in \mathbb{R}^{3 \times 3}\)&nbsp;, camera <b>extrinsics</b> (<i>world-to-camera</i>) \(P \in \mathbb{R}^{4 \times 4}\) and <b>depthmap</b> \(D \in \mathbb{R}^{W\times H}\).<br>pointmap \(X\) can be obtained by \(X_{i,j} = K^{-1} ([i D_{i,j}, j D_{i,j}, D_{i,j})\), where \(X\) is expressed in the camera coordinate frame.<br>To express pointmap \(X^n\)&nbsp;from camera \(n\) in camera \(m\)'s coordinate frame:&nbsp; &nbsp; &nbsp;\(X^{n,m} = P_m P_n^{-1} X^n\).",DUSt3R: Geometric 3D Vision Made Easy,https://arxiv.org/abs/2312.14132,,
n$qikp+YtL,Which <b>loss function</b> is used to train <b>DUSt3R</b>?,"<b>Confidence-aware</b>&nbsp;<b>3D Regression loss</b>.<br>Given the ground-truth pointmaps \(\bar{X}^{1,1}\) and \(\bar{X}^{2,1}\) along with two corresponding sets of valid pixels \(\mathcal{D}^1,\mathcal{D}^2 \subseteq \{1\ldots W\}\times\{1\ldots H\}\) on which the ground-truth is defined.<br>The <b>regression loss</b> for a valid pixel \(i\in\mathcal{D}^v\) in view \(v\in\{1,2\}\) is <b>simply defined as the Euclidean distance</b>:<br>\[\mathcal{l}_{\text{reg}}(v,i) = (\| \frac{1}{z}X^{v,1}_{i} - \frac{1}{\bar{z}}\bar{X}^{v,1}_{i} )\|.\]<br>To handle the scale ambiguity between prediction and ground-truth, the predicted and ground-truth pointmaps are normalized by scaling factors \(z=\operatorname{norm}(X^{1,1},X^{2,1})\) and \(\bar{z}=\operatorname{norm}(\bar{X}^{1,1},\bar{X}^{2,1})\), respectively, which simply represent the average distance of all valid points to the origin:<br>\[\operatorname{norm}(X^1,X^2) = \frac{1}{|\mathcal{D}^1| + |\mathcal{D}^2|} \sum_{v \in \{1,2\}} \sum_{i \in \mathcal{D}^v} \| X^v_{i} \|\]As some parts of the image are harder to predict than others, the network also predicts a score for each pixel which represents the confidence that the network has about this particular pixel.<br><b>The final training objective is the confidence-weighted regression loss over all valid pixels:</b><br>\[\mathcal{L}_{\text{conf}} = \sum_{v \in \{1,2\}} \, \sum_{i \in \mathcal{D}^v}  C^{v,1}_i \mathcal{l}_{\text{reg}}(v,i) - \alpha \log C^{v,1}_i\]<br>where \(C^{v,1}_i\) is the confidence score for pixel \(i\), and \(\alpha\) is a hyper-parameter controlling the regularization.<br>To ensure a strictly positive confidence, they define <br>\(C^{v,1}_i=1+\exp \widetilde{C^{v,1}_i} &gt;1\).<br>This has the effect of forcing the network to extrapolate in harder areas, e.g. like those ones covered by a single view.<br>Training the network with this objective allows to estimate confidence scores without an explicit supervision.<br>",DUSt3R: Geometric 3D Vision Made Easy,https://arxiv.org/abs/2312.14132,,
