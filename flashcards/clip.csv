guid,question,answer,paper_title,paper_url,explanation,tags
CeybD~r>=%,Give a summary of the approach used in <b>CLIP</b>.,"<img src=""paste-39fa726bd182ceab62978bfde49ed398a1c02ba7.jpg""><br>Given a batch of \(N\) (image, text) pairs, CLIP learns a multi-model embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the \(N\)&nbsp;correct pairs while minimizing the cosine similarity of the \(N^2 - N\) incorrect pairs. (It is optimized using a symmetric cross entropy loss over these similarity scores)",Learning Transferable Visual Models From Natural Language Supervision,https://arxiv.org/abs/2103.00020,,
Jr:$:]5$=U,"How many (image, text) pairs were collected in the dataset used to train <b>CLIP</b>?",400 million,Learning Transferable Visual Models From Natural Language Supervision,https://arxiv.org/abs/2103.00020,,
