guid,question,answer,paper_title,paper_url,explanation,tags
n*aVp#3e.W,"What is the formula for&nbsp;<b>variational lower bound</b>, or <b>evidence lower bound</b> (ELBO) in variational Bayesian methods?",The evidence lower bound (ELBO) is defined as&nbsp;<br>\[\log p_\theta(\mathbf{x}) - D_\text{KL}( q_\phi(\mathbf{z}\vert\mathbf{x}) \| p_\theta(\mathbf{z}\vert\mathbf{x}) ) = \mathbb{E}_{\mathbf{z}\sim q_\phi(\mathbf{z}\vert\mathbf{x})}\log p_\theta(\mathbf{x}\vert\mathbf{z}) - D_\text{KL}(q_\phi(\mathbf{z}\vert\mathbf{x}) \| p_\theta(\mathbf{z}))\]<br>The&nbsp;<i>lower bound</i>&nbsp;part in the name comes from the fact that the KL divergence is always non-negative and thus ELBO is the lower bound of \(\log p_\theta (\mathbf{x})\).,Auto-Encoding Variational Bayes,https://arxiv.org/abs/1312.6114,"Additional sources:&nbsp;<a href=""https://lilianweng.github.io/posts/2018-08-12-vae"">https://lilianweng.github.io/posts/2018-08-12-vae</a>,&nbsp;<a href=""https://blog.evjang.com/2016/08/variational-bayes.html"">https://blog.evjang.com/2016/08/variational-bayes.html</a>",
kqf`=P<d<W,<b>Derive</b> the <b>ELBO</b> loss function used for <b>variational inference</b>.,"The goal in variational inference is to <b>minimize</b> \(D_\text{KL}( q_\phi(\mathbf{z}\vert\mathbf{x}) | p_\theta(\mathbf{z}\vert\mathbf{x}) )\) with respect to \(\phi\):<br>\[\begin{aligned}
&amp; D_\text{KL}( q_\phi(\mathbf{z}\vert\mathbf{x}) \| p_\theta(\mathbf{z}\vert\mathbf{x}) ) &amp; \\
&amp;=\int q_\phi(\mathbf{z} \vert \mathbf{x}) \log\frac{q_\phi(\mathbf{z} \vert \mathbf{x})}{p_\theta(\mathbf{z} \vert \mathbf{x})} d\mathbf{z} &amp; \\
&amp;=\int q_\phi(\mathbf{z} \vert \mathbf{x}) \log\frac{q_\phi(\mathbf{z} \vert \mathbf{x})p_\theta(\mathbf{x})}{p_\theta(\mathbf{z}, \mathbf{x})} d\mathbf{z} &amp; \scriptstyle{\text{; Because }p(z \vert x) = p(z, x) / p(x)} \\
&amp;=\int q_\phi(\mathbf{z} \vert \mathbf{x}) \big( \log p_\theta(\mathbf{x}) + \log\frac{q_\phi(\mathbf{z} \vert \mathbf{x})}{p_\theta(\mathbf{z}, \mathbf{x})} \big) d\mathbf{z} &amp; \\
&amp;=\log p_\theta(\mathbf{x}) + \int q_\phi(\mathbf{z} \vert \mathbf{x})\log\frac{q_\phi(\mathbf{z} \vert \mathbf{x})}{p_\theta(\mathbf{z}, \mathbf{x})} d\mathbf{z} &amp; \scriptstyle{\text{; Because }\int q(z \vert x) dz = 1}\\
&amp;=\log p_\theta(\mathbf{x}) + \int q_\phi(\mathbf{z} \vert \mathbf{x})\log\frac{q_\phi(\mathbf{z} \vert \mathbf{x})}{p_\theta(\mathbf{x}\vert\mathbf{z})p_\theta(\mathbf{z})} d\mathbf{z} &amp; \scriptstyle{\text{; Because }p(z, x) = p(x \vert z) p(z)} \\
&amp;=\log p_\theta(\mathbf{x}) + \mathbb{E}_{\mathbf{z}\sim q_\phi(\mathbf{z} \vert \mathbf{x})}[\log \frac{q_\phi(\mathbf{z} \vert \mathbf{x})}{p_\theta(\mathbf{z})} - \log p_\theta(\mathbf{x} \vert \mathbf{z})] &amp;\\
&amp;=\log p_\theta(\mathbf{x}) + D_\text{KL}(q_\phi(\mathbf{z}\vert\mathbf{x}) \| p_\theta(\mathbf{z})) - \mathbb{E}_{\mathbf{z}\sim q_\phi(\mathbf{z}\vert\mathbf{x})}\log p_\theta(\mathbf{x}\vert\mathbf{z}) &amp;
\end{aligned}\]So we have:<br>\[D_\text{KL}( q_\phi(\mathbf{z}\vert\mathbf{x}) \| p_\theta(\mathbf{z}\vert\mathbf{x}) ) =\log p_\theta(\mathbf{x}) + D_\text{KL}(q_\phi(\mathbf{z}\vert\mathbf{x}) \| p_\theta(\mathbf{z})) - \mathbb{E}_{\mathbf{z}\sim q_\phi(\mathbf{z}\vert\mathbf{x})}\log p_\theta(\mathbf{x}\vert\mathbf{z})\]<br><div>Once rearrange the left and right hand side of the equation,</div><div>\[\log p_\theta(\mathbf{x}) - D_\text{KL}( q_\phi(\mathbf{z}\vert\mathbf{x}) \| p_\theta(\mathbf{z}\vert\mathbf{x}) ) = \mathbb{E}_{\mathbf{z}\sim q_\phi(\mathbf{z}\vert\mathbf{x})}\log p_\theta(\mathbf{x}\vert\mathbf{z}) - D_\text{KL}(q_\phi(\mathbf{z}\vert\mathbf{x}) \| p_\theta(\mathbf{z}))\]</div><div>The left side of the equation is&nbsp;exactly what we want to maximize when learning the true distributions: we want to maximize the (log-)likelihood of generating real data (that is \(\log p_\theta(\mathbf{x})\)) and also minimize the difference between the real and estimated posterior distributions (the term \(D_\text{KL}\) works like a regularizer). Note that \(p_\theta(\mathbf{x})\) is fixed with respect to \(q_\phi\).</div><div><br></div><div><div>The negation of the above defines our <b>loss function</b>:</div></div><div>\[\begin{aligned}
L_\text{VAE}(\theta, \phi) 
&amp;= -\log p_\theta(\mathbf{x}) + D_\text{KL}( q_\phi(\mathbf{z}\vert\mathbf{x}) \| p_\theta(\mathbf{z}\vert\mathbf{x}) )\\
&amp;= - \mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z}\vert\mathbf{x})} \log p_\theta(\mathbf{x}\vert\mathbf{z}) + D_\text{KL}( q_\phi(\mathbf{z}\vert\mathbf{x}) \| p_\theta(\mathbf{z}) ) \\
\theta^{*}, \phi^{*} &amp;= \arg\min_{\theta, \phi} L_\text{VAE}
\end{aligned}\]</div>",Auto-Encoding Variational Bayes,https://arxiv.org/abs/1312.6114,"<img src=""VAE-graphical-model.png""><br>The graphical model involved in <b>Variational Autoencoder.</b> Solid lines denote the generative distribution \(p_\theta(.)\) and dashed lines denote the distribution \(q_\phi(z \mid x)\) to approximate the intractable posterior \(p_\theta(z \mid x)\).<br>Source:&nbsp;<a href=""https://lilianweng.github.io/posts/2018-08-12-vae/"">https://lilianweng.github.io/posts/2018-08-12-vae/</a>",
"MS%/5nz,@3",Why does <b>ELBO</b> use the <b>reverse Kullback-Leibler divergence</b> \(D_\text{KL}(q_\phi | p_\theta)\) instead of the forward KL divergence \(D_\text{KL}(p_\theta | q_\phi)\)?,"KL divergence is not&nbsp;a symmetric distance function, i.e. \(D_\text{KL}(q_\phi | p_\theta) \ne D_\text{KL}(p_\theta | q_\phi)\).<br>Let's <b>consider the forward KL divergence</b>:<br>\[\begin{align*} D_\text{KL}(p | q) &amp; = \sum_z p(z) \log \frac{p(z)}{q(z)} \\ &amp; = \mathbb{E}_{p(z)}{\big[\log \frac{p(z)}{q(z)}\big]}\\ \end{align*}\]This means that we need to ensure that \(q(z) &gt; 0\) wherever \(p(z) &gt; 0\). The optimized variational distribution \(Q(Z)\) is known as <b>zero-avoiding</b>.<br><img src=""forward-KL.png""><br>The reversed KL divergence has the opposite behaviour.<br>\[\begin{align*} D_\text{KL}(q | p) &amp; = \sum_z p(z) \log \frac{q(z)}{p(z)} \\ &amp; = \mathbb{E}_{q(z)}{\big[\log \frac{q(z)}{p(z)}\big]}\\ \end{align*}\]<br>If \(p(z) = 0\), we must ensure that \(q(z) = 0\), othewise the KL divergence blows up. This is known as <b>zero-forcing</b>.<br><img src=""reverse-KL.png""><br>",Auto-Encoding Variational Bayes,https://arxiv.org/abs/1312.6114,"Source:&nbsp;<a href=""https://blog.evjang.com/2016/08/variational-bayes.html"">https://blog.evjang.com/2016/08/variational-bayes.html</a>",
"p,Fi]o-OIJ",What is the <b>reparameterization trick</b> used in <b>variational autoencoders</b>?,"Variational autoencoders sample from \(\mathbf{z} \sim q_\phi(\mathbf{z}\vert\mathbf{x})\).<br>Sampling is a stochastic process and therefore we <b>cannot backpropagate through it</b>.<br><b>To make it differentiable, the reparameterization trick is introduced.<br></b>It is often possible to express the random variable \(\mathbf{z}\) as a deterministic variable \(\mathbf{z} = \mathcal{T}_\phi(\mathbf{x}, \boldsymbol{\epsilon})\), where \(\epsilon\) is an auxiliary independent random variable and the transformation function \(\mathcal{T}_\phi\) converts \(\boldsymbol{\epsilon}\) to \(\mathbf{z}\).<br><br>For example, a common choice of the form of \(q_\phi(\mathbf{z}\vert\mathbf{x})\) is a multivariate Gaussian with a diagonal covariance structure:<br>\[\begin{aligned}
\mathbf{z} &amp;\sim q_\phi(\mathbf{z}\vert\mathbf{x}^{(i)}) = \mathcal{N}(\mathbf{z}; \boldsymbol{\mu}^{(i)}, \boldsymbol{\sigma}^{2(i)}\boldsymbol{I}) &amp; \\
\mathbf{z} &amp;= \boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon} \text{, where } \boldsymbol{\epsilon} \sim \mathcal{N}(0, \boldsymbol{I}) &amp; \scriptstyle{\text{; Reparameterization trick.}}
\end{aligned}\]<br><img src=""reparameterization-trick.png""><br>",Auto-Encoding Variational Bayes,https://arxiv.org/abs/1312.6114,"Source:&nbsp;<a href=""https://lilianweng.github.io/posts/2018-08-12-vae/"">https://lilianweng.github.io/posts/2018-08-12-vae/</a>",
q0q-k?k=TH,Draw the architecture of a <b>variational autoencoder</b> (<b>VAE</b>).,"<img src=""vae-gaussian.png"">",Auto-Encoding Variational Bayes,https://arxiv.org/abs/1312.6114,"Source:&nbsp;<a href=""https://lilianweng.github.io/posts/2018-08-12-vae/"">https://lilianweng.github.io/posts/2018-08-12-vae/</a>",
