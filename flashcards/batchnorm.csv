guid,question,answer,paper_title,paper_url,explanation,tags
Mk%J-^b79.,How does <b>Batch Normalization</b>&nbsp;work?,"<b>Batch Normalization</b>&nbsp;normalizes <b>each scalar feature independently</b>, by making it have <b>zero mean</b>&nbsp;and a <b>variance of 1</b>. The normalized values are then <b>scaled and shifted</b>.<br><u>During training</u>, the batchnorm layer works as follows:<br>
<hr>
<b>Input:</b>&nbsp;Values of \(x\) over a mini-batch: \(\mathcal{B} = {x_{1...m}}\)<br><b>Output:</b>&nbsp;\(y_i = \operatorname{BatchNorm}_{\gamma, \beta}(x_i)\)<br>\[\mu_{\mathcal{B}} = \frac{1}{m} \sum^m_{i=1}x_i\]<br>\[\sigma^2_{\mathcal{B}} =&nbsp;\frac{1}{m}&nbsp;\sum^m_{i=1}(x_i - \mu_{\mathcal{B}})^2\]<br>\[\hat{x}_i = \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma^2_{\mathcal{B}} + \epsilon}} \]<br>\[y_i = \gamma x_i + \beta\]<br><hr>With \(\epsilon\) a constant added for numeric stability.<br>And <b>\(\gamma\) and \(\beta\) learned parameters</b>.<br><u>During inference</u>, the normalization is done using the population rather than mini-batches.<br>\[\hat{x}_i = \frac{x_i - \operatorname{E}[x_i]}{\sqrt{\operatorname{Var}[x_i] + \epsilon}} \]<br>",Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,https://arxiv.org/abs/1502.03167,,
m>D}7b&hT(,What is different for using <b>Batch Normalization</b>&nbsp;in <b>Convolutions</b>&nbsp;compared to <b>fully connected layers</b>?,"For convolutional layers, we additionally want the normalization to obey the <b>convolutional property</b> â€“ so that different elements of the same feature map, at <b>different locations, are normalized in the same way</b>. To achieve this, we <b>jointly normalize all the activations in a minibatch, over all locations</b>.<br>In other words, in convolutions <b>we normalize (and scale/shift) per feature map (channel), rather&nbsp;than per individual value</b>.",Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,https://arxiv.org/abs/1502.03167,,
E2Fv}l-i?e,Why <b>does Batch Normalization</b> have <b>learned scale and shift parameters</b>?,The learned shift and scale parameters <b>\(\gamma\)</b> and <b>\(\beta\)</b> in&nbsp;<br>\[y_k = \gamma_k \hat{x}_k + \beta_k\]<br><b>enable the full represation power of the neural network.</b>,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,https://arxiv.org/abs/1502.03167,"By setting \(\gamma_k = \sqrt{\operatorname{Var}[x_k]}\) and \(\beta_k = \operatorname{E}[x_k]\), we <b>could recover the original values</b>, if that were the optimal thing to do.",
