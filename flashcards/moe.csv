guid,question,answer,paper_title,paper_url,explanation,tags
"ua,`ueVi>=",Draw an overview of the <b>Sparsely Gated Mixture of Experts (MoE)</b> layer.,"<img src=""moe.png""><br>The Mixture-of-Experts layer consists of <i>\(n\) experts</i> (simple feed forward layers in the original paper) and a&nbsp;<i>gating network </i>\(G\)<i>&nbsp;</i>whose output is a sparse \(n\) dimensional vector.<br><br>Let us denote by \(G(x)\) and \(E_i(x)\) the output of the gating network and the output of the \(i\)-th expert network for a given input \(x\). The output \(y\) of the MoE module can be written as follows:<br>\[y = \sum_{i=1}^n G(x)_iE_i(x)\]We save computation based on the sparsity of the output of \(G(x)\).",Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,https://arxiv.org/abs/1701.06538,,
"o%Qf%X,,_?",Give the structure of the&nbsp;<i>gating network \(G\)</i>&nbsp;that is used in Sparsely Gated MoE.,"A simple choice of \(G\) is to multiply the input with a trainable weight matrix \(W_g\) and then apply a&nbsp;\(\operatorname{softmax}\): \(G_\sigma (x) = \operatorname{softmax}(x W_g)\).<br><br>However we want a&nbsp;<i>sparsely</i>&nbsp;gated MoE where we only evaluate the top-\(k\) experts.<br>The sparsity serves to save computations.<br>Thus the MoE layer only keeps the top-\(k\) values:<br>\[\begin{aligned}
G(x) &amp;= \operatorname{softmax}(\operatorname{KeepTopK}(H(x),k)) \\
\operatorname{KeepTopK}(v,k)_i &amp;=\begin{cases} v_i &amp; \text{if }v_i\text{ is in the top }k\text{ elements of }v \\ -\infty &amp; \text{otherwise} \end{cases}
\end{aligned}\]and where \(H(x)\) is a linear layer with added tunable Guassian noise such that each expert sees enough training data and we avoid favouring only a few experts for all inputs.<br>\[H(x)_i = (xW_g)_i + \epsilon \cdot \operatorname{softplus}((xW_\text{noise})_i ); \quad \epsilon \sim \mathcal{N}(0, \mathbf{1})\]&nbsp;<br>",Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,https://arxiv.org/abs/1701.06538,,
d?iOurkObo,What is the&nbsp;<b>shrinking batch problem</b>&nbsp;in <b>MoE</b>s?,"If a MoE uses only \(k\) out of \(n\) experts, then for a batch of size \(b\), each export only receive approximately \(\frac{kb}{n} \ll b\) samples.",Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,https://arxiv.org/abs/1701.06538,Through data and model parallelism this problem can be negated.,
C+^v0fx*:n,How does the Sparsely-Gated <b>MoE</b> paper <b>avoid</b> the <b>gating network \(G\) </b>to <b>always favor the same</b> few strong experts?,"They soft constrain the learning with an additional importance loss \(\mathcal{L}_{\text{importance}}\) that encourages all experts to have equal importance.<br>Where importance is defined as: \(\operatorname{Importance}(X) = \sum_{x \in X} G(x)\)<br>The importance loss is then defined as the <i>coefficient of variation</i> of the batchwise average importance per expert:<br>\[\mathcal{L}_{\text{importance}} = \lambda \cdot \operatorname{CV}(\operatorname{Importance}(X))^2\]Where the coefficient of variation CV is defined as&nbsp;the ratio of the standard deviation \(\sigma\) to the mean \(\mu\), \(\operatorname{CV} = \frac{\sigma}{\mu}\).",Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,https://arxiv.org/abs/1701.06538,,
vth}x8pH40,In the Sparsely gated <b>MoE</b> paper what is the <b>load loss</b> \(\mathcal{L}_{\text{load}}\) and what does it encourage?,<div>The load loss \(\mathcal{L}_{load}\) encourages equal load per expert. It uses a smooth estimator \(Load(X)\) of examples per expert and minimizes:</div><div>\[\mathcal{L}_{load}(X) = \lambda_{load} \cdot \operatorname{CV}(Load(X))^2\]</div>,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,https://arxiv.org/abs/1701.06538,"For the full calculation of the load value, check the original paper.",
