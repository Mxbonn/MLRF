guid,question,answer,paper_title,paper_url,explanation,tags
zr_]ryjRHx,What are <b>Slimmable Neural Networks</b>?,"A Slimmable Neural Network is a <b>single</b>&nbsp;neural network executable at <b>different widths</b>&nbsp;(number of active channels).<div><img src=""paste-b41b6392ef1254058e87d648f8e04f17b0f3f067.jpg""><br></div>",Slimmable Neural Networks,https://openreview.net/forum?id=H1gMCsAqY7,This permits <b>instant and adaptive</b>&nbsp;accuracy-efficiency trade-offs <b>at runtime</b>.,
Nn6:bX/%pl,What is the <b>training algorithm</b> to train <b>Slimmable Neural Networks</b>?,"<img src=""paste-4619681eabe23c869f41b376f0ae8e235495e51a.jpg"">",Slimmable Neural Networks,https://openreview.net/forum?id=H1gMCsAqY7,Or in other words: apply each batch over the slimmed networks (which share weights for each layer but have an individual batchnorm layer) and accumulate the loss. Do a weight update at the end.<div><br></div>,
dU@YsQb@b-,What was the<b> main difficulty</b> to make <b>Slimmable Neural Networks</b> work and how was it solved?,"For each layer, different channels/switches result in different means and variances of the aggregated feature, which are then rolling averaged to a shared batch normalization layer. The inconsistency leads to <b>inaccurate batch normalization statistics</b> in a layer-by-layer propagating manner.<div><br><div>Note that these batch normalization&nbsp;statistics (moving averaged means and variances) are only used during testing, in training the means and variances of the current mini-batch are used.</div></div><div><br></div><div>The solution was the introduction of<b>&nbsp;Switchable Batch Normalization (S-BN)</b>, that employs independent batch normalization for different switches in a slimmable network.</div>",Slimmable Neural Networks,https://openreview.net/forum?id=H1gMCsAqY7,"<img src=""paste-2831e9519fc33eba4fb0bb50166a0c45efdd85ae.jpg"">",
