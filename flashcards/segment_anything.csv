guid,question,answer,paper_title,paper_url,explanation,tags
LyZ(:+v<1e,How long did it take to train<b>&nbsp;</b>the&nbsp;<b>Segment Anything</b>&nbsp;model?,SAM was trained on 256 A100 GPUS for 68 hours. (This is equal to 725 A100 GPU-days),Segment Anything,https://arxiv.org/abs/2304.02643,They acknowledge the environmental impact and cost of training&nbsp;large scale models. The environmental impact of training the released SAM model is approximately 6963 kWh&nbsp;resulting in an estimated 2.8 metric tons of carbon dioxide given the specific data center used.&nbsp;This is equivalent to ∼7k miles driven by the average&nbsp;gasoline-powered passenger vehicle in the US.&nbsp;,
f@[z]27*~S,What <b>Image encoder</b> is used in the&nbsp;<b>Segment Anything</b>&nbsp;model?,"They use an <b>MAE</b> (Masked auto-encoder) pretrained <b>ViT</b> (Vision Transformer).<br>Specifically a <b>ViT-H/16</b> with \(14 \times 14\) windowed attention&nbsp;and four equally-spaced global attention blocks.<br>Following standard practices, they use an input resolution of \(1024 \times 1024\) obtained by rescaling the image and padding the shorter side. The image embedding&nbsp;is therefore \(64\times 64\). To <b>reduce the channel dimension</b>, they use a \(1 \times 1\) convolution to get to 256 channels, followed by a \(3 \times 3\) convolution also with 256 channels.&nbsp;Each convolution is followed by a layer normalization.",Segment Anything,https://arxiv.org/abs/2304.02643,The windowed attention is used to handle high resolution inputs as the window size (i.e. \(14 \times 14\)) matches the full resolution of ImageNet training images.,
L+x%neuGT,Give a rough schematic of the <b>Segment Anything</b> Model.,"<img src=""338558258_1349701259095991_4358060436604292355_n.png""><br><b>SAM</b> has three components: <b>an image encoder, a flexible prompt&nbsp;encoder, and a fast mask decoder.</b>&nbsp;<br><br><b>Image encoder</b>:&nbsp;&nbsp;an <b>MAE pre-trained Vision&nbsp;Transformer (ViT)</b>, minimally adapted to process high&nbsp;resolution inputs.<br><br><b>Prompt encoder</b>:&nbsp; two sets of prompts: sparse&nbsp;(points, boxes, text) and dense (masks). They represent&nbsp;points and boxes by <b>positional encodings summed with&nbsp;learned embeddings</b> for each prompt type and <b>free-form text&nbsp;with an off-the-shelf text encoder from CLIP</b>. <b>Dense<br>prompts (i.e., masks) are embedded using convolutions and&nbsp;summed element-wise with the image embedding</b>.<br><br><b>Mask decoder</b>: The&nbsp;design employs a modification of a Transformer decoder block followed&nbsp;by a dynamic mask prediction head. The modified decoder&nbsp;block uses <b>prompt self-attention and cross-attention in two&nbsp;directions</b> (prompt-to-image embedding and vice-versa) to&nbsp;update all embeddings. After running two blocks, they upsample the image embedding and an<b> MLP maps the output&nbsp;token to a dynamic linear classifier</b>, which then computes&nbsp;the mask foreground probability at each image location.",Segment Anything,https://arxiv.org/abs/2304.02643,,
B54yd#jc!_,What is the architecture of the&nbsp;<i>lightweight</i>&nbsp;<b>mask decoder</b> used in <b>Segment Anything</b>?,"<img src=""paste-42cc0c0476bbaf888c3ec3d4a664e6c635714f8a.jpg""><br>The decoder is a <b>modified Transformer decoder</b> (similar to DETR and MaskFormer).<br>First insert into the set of prompt embeddings&nbsp;a learned output token embedding that will be used at the&nbsp;decoder’s output, analogous to the [class] token.<br>For simplicity, we refer to these embeddings (not including&nbsp;the image embedding) collectively as <i>tokens</i>.<br><br>Each decoder&nbsp;layer performs 4 steps: <br><b>(1) self-attention on the tokens</b>, <b>(2)&nbsp;cross-attention</b> from tokens (as queries) to the image embedding, <b>(3) a point-wise MLP</b> updates each token, and <b>(4)&nbsp;cross-attention</b> from the image embedding (as queries) to&nbsp;tokens. <br>This last step updates the image embedding with&nbsp;prompt information. During cross-attention, the image embedding is treated as a set of \(64^2\) 256-dimensional vectors.&nbsp;Each self/cross-attention and MLP has a residual connection, layer normalization, and a dropout of 0.1 at&nbsp;training. The next decoder layer takes the updated tokens&nbsp;and the updated image embedding from the previous layer. <br>They use a <b>two-layer decoder</b>.&nbsp;To ensure the decoder has access to critical geometric information the <u>positional encodings are added to the image&nbsp;embedding whenever they participate in an attention layer</u>.&nbsp;Additionally, <b>the entire original prompt tokens (including<br>their positional encodings) are re-added to the updated tokens whenever they participate in an attention layer</b>. This&nbsp;allows for a strong dependence on both the prompt token’s&nbsp;geometric location and type.<br><br><b>After the decoder, we upsample</b> the updated image embedding by 4x with two transposed convolutional layers.<br>Then, the<b>&nbsp;tokens attend once more to the image embedding</b>&nbsp;and we pass the updated <b>output token embedding to a small&nbsp;3-layer MLP</b> that outputs a vector matching the channel dimension of the upscaled image embedding. Finally, we <b>predict a mask with a spatially point-wise product between the&nbsp;upscaled image embedding and the MLP’s output</b>.<br>The transformer uses an embedding dimension of 256.<br>The transformer MLP blocks have a large internal dimension of 2048, but the MLP is applied only to the prompt tokens for which there are relatively few (rarely greater than&nbsp;20). However, in cross-attention layers where we have a \(64 \times 64\) image embedding, we reduce the channel dimension&nbsp;of the queries, keys, and values by \(2 \times\) to 128 for computational efficiency. All attention layers use 8 heads.&nbsp;The transposed convolutions used to upscale the output&nbsp;image embedding are \(2 \times 2\), stride 2 with output channel dimensions of 64 and 32 and have GELU activations. They&nbsp;are separated by layer normalization.",Segment Anything,https://arxiv.org/abs/2304.02643,,
uiv>P3<.Np,What is the architecture of the <b>prompt encoder</b> used in&nbsp;<b>Segment Anything</b>?,"Sparse (i.e. not the mask) <b>prompts are mapped to 256-dimensional vectorial embeddings</b> as follows. <br><b>A point </b>is&nbsp;represented as the <b>sum of a positional encoding of the&nbsp;point’s location and one of two learned embeddings</b> that indicate if the point is either in the foreground or background.<br><b>A box </b>is represented by an embedding pair: (1) the <b>positional encoding of its top-left corner</b> summed with a <b>learned&nbsp;embedding</b> representing “top-left corner” and (2) the same&nbsp;structure but using a learned embedding indicating “bottom-right corner”. <br><b>For text</b> we use the&nbsp;text encoder from <b>CLIP</b>.<br><br><b>Dense prompts (i.e., masks)</b> have a spatial correspondence with the image. They input masks at a \(4 \times\) lower resolution than the input image, then <b>downscale</b> an additional \(4 \times\)&nbsp;using two \(2 \times 2\), stride-2 convolutions with output channels 4 and 16, respectively. A final \(1 \times 1\)&nbsp;convolution maps&nbsp;the channel dimension to 256. Each layer is separated by&nbsp;GELU activations and layer normalization.<br><br><b>The mask and image embedding are then added element-wise</b>. If there&nbsp;is no mask prompt, a <b>learned embedding representing “no&nbsp;mask”</b> is added to each image embedding location.",Segment Anything,https://arxiv.org/abs/2304.02643,,
nT}J]!}x5M,For which task is the <b>Segment Anything</b> Model trained?,"<b>The promptable segmentation task</b>, which&nbsp;returns a valid segmentation mask given any <b>prompt</b>. Where a prompt can be a set of foreground&nbsp;/ background points, a rough box or mask, free-form text,&nbsp;or, in general, any information indicating what to segment&nbsp;in an image.<br>The requirement of a “<i>valid</i>” mask simply means that even when&nbsp;a prompt is <u>ambiguous</u> and could refer to multiple objects&nbsp;the output should be a reasonable mask for at least one of&nbsp;those objects.",Segment Anything,https://arxiv.org/abs/2304.02643,,
IqXIh{Om/s,Which <b>losses</b> were used to train <b>Segment Anything</b>?,A linear combination of <b>focal loss</b>&nbsp;and <b>dice loss </b>in a 20:1 ratio of&nbsp;focal loss to dice loss.,Segment Anything,https://arxiv.org/abs/2304.02643,,
g=8Z]w0ehK,How can the <b>Segment Anything</b> model be used for <b>instance segmentation</b>?,It needs to be <b>combined with an object detector</b> which provides the bounding box prompt for Segment Anything.,Segment Anything,https://arxiv.org/abs/2304.02643,,
