guid,question,answer,paper_title,paper_url,explanation,tags
s/Q#o+#&?I,What is a <b>Once-for-all Network</b>?,"A once-for-all&nbsp;network is a neural network that can be <b>directly deployed</b> under <b>diverse architecture</b> configurations, amortizing the training cost.&nbsp;Given a deployment scenario, a specialized subnetwork is directly selected from the once-for-all network without training.",Once-for-All: Train One Network and Specialize it for Efficient Deployment<br>,https://arxiv.org/abs/1908.09791,A once-for-all network maintains good accuracy on a large number of sub-networks (more than \(10^{19}\)),
9bw[%Mc*D,Which dimensions are scalable in a <b>once-for-all network</b>?,"The <b>depth, width, kernel size, </b>and <b>resolution</b>.",Once-for-All: Train One Network and Specialize it for Efficient Deployment<br>,https://arxiv.org/abs/1908.09791,"An OFA network is constructed by&nbsp; dividing a CNN model into a sequence of units with gradually reduced feature map size and increased channel numbers.<div>Each unit is allowed to use arbitrary numbers of layers, each layer to use arbitrary numbers of channels and arbitrary kernel sizes, and the model is also allowed to take arbitrary input image sizes.&nbsp;</div><div>Although, <i>arbitrary</i>&nbsp;is maybe a strong word: in their experiments, the input image size ranges from 128 to 224 with a stride 4; the depth of each unit is chosen from {2, 3, 4}; the width expansion ratio in each layer is chosen from {3, 4, 6}; the kernel size is chosen from {3, 5, 7}.</div>",
k=^XvJ08rK,How is a <b>once-for-all</b>&nbsp;network <b>trained?</b>,"By using a <b>progessive shrinking </b>training scheme.<div>Start with training the largest neural network with the maximum kernel size (e.g., 7), depth (e.g., 4), and width (e.g., 6). Next, progressively fine-tune the network to support smaller sub-networks by gradually adding them into the sampling space (larger sub-networks may also be sampled).&nbsp;</div><div>Specifically, after training the largest network, first support elastic kernel size, while the depth and width remain the maximum values. Then, support elastic depth and elastic width sequentially. The resolution is elastic throughout the whole training process, which is implemented by sampling different image sizes for each batch of training data. We also use the <b>knowledge distillation</b> technique after training the largest neural network. It combines two loss terms using both the soft labels&nbsp;given by the largest neural network and the real labels.<br></div>",Once-for-All: Train One Network and Specialize it for Efficient Deployment<br>,https://arxiv.org/abs/1908.09791,"<img src=""paste-101f93c2ca334ae597210b15c02b5fe4ddbbcb42.jpg"">",
I@=vdhY6:0,How are&nbsp;<b>once-for-all</b>&nbsp;networks able to significantly reduce the time for NAS compared to previous work?,"They <b>decouple model training from neural architecture search</b>. Or in other words, the search phase does not require any training.",Once-for-All: Train One Network and Specialize it for Efficient Deployment<br>,https://arxiv.org/abs/1908.09791,,
f6I|bNc6z~,"Starting from a <b>once-for-all network</b>, how do you select a specialized sub-network for a given deployment scenario?","Apart from the OFA network, you also need to construct<b> an accuracy predictor</b> (a small MLP trained on 16K sub-networks and their accuracy measured over 10K validation images) and a <b>latency lookup table</b>.<div>Given the target hardware and latency constraint, you conduct an <b>evolutionary search</b> to get the specialized sub-networks.</div><div><img src=""paste-8daf65321c6185ed884b64102155e2e06565132a.jpg""><br></div>",Once-for-All: Train One Network and Specialize it for Efficient Deployment<br>,https://arxiv.org/abs/1908.09791,Getting the data for the accuracy predictor (i.e. the sub-network and it's accuracy) takes 40 GPU hours.,
Q@eP(4|w4#,How many hours does it take to train a <b>once-for-all network</b>&nbsp;(according to the experiments in the paper)?,"Around <b>1,200</b>&nbsp;GPU hours on V100 GPUs.",Once-for-All: Train One Network and Specialize it for Efficient Deployment<br>,https://arxiv.org/abs/1908.09791,+ <b>40</b>&nbsp;GPU hours for the accuracy predictor.,
fPoTT}O5C5,How does <b>Elastic Kernel Size</b>&nbsp;work in <b>once-for-all networks</b>?,"<div><b>The center of a \(7 \times 7\) convolution kernel also serves as a 5x5 kernel, the center of which also serves to be a 3x3 kernel.&nbsp;</b></div><div>The weights of centered sub-kernels may need to have different distribution or magnitude as different roles. <u>Forcing them to be the same degrades the performance of some sub-networks. </u>Therefore, we introduce <b>kernel transformation matrices </b>when sharing the kernel weights. We use <b>separate kernel transformation matrices for different layers</b>. Within each layer, the kernel transformation matrices are <b>shared among different channels</b>. As such, we only need \(25 \times 25 + 9 \times 9 = 706\) extra parameters to store&nbsp;the kernel transformation matrices in each layer.<br></div><img src=""paste-a89e3a046d5bc158425199e8f62ad8ad3eb5bd3f.jpg"">",Once-for-All: Train One Network and Specialize it for Efficient Deployment<br>,https://arxiv.org/abs/1908.09791,,
ta/{tZ4IEE,How does&nbsp;<b>Elastic Depth</b>&nbsp;work in&nbsp;<b>once-for-all networks</b>?,"To derive a sub-network that has D layers in a unit that originally has N layers, we<b> keep the first D layers and skip the last N −D layers</b>.",Once-for-All: Train One Network and Specialize it for Efficient Deployment<br>,https://arxiv.org/abs/1908.09791,,
J[)r1z52=m,How does <b>Elastic Width</b>&nbsp;work in <b>once-for-all networks</b>?,"<img src=""paste-0f041094e5418ebe4e8f3104337ef81715f9d51b.jpg""><div>It uses a <b>channel sorting</b> operation which reorganizes the channels according to their importance, which is calculated based on the <b>L1 norm</b> of a channel’s weight.&nbsp;<br></div>",Once-for-All: Train One Network and Specialize it for Efficient Deployment<br>,https://arxiv.org/abs/1908.09791,"Thereby, smaller sub-networks are initialized with the most important channels on the once-for-all network which is already well&nbsp;trained. This channel sorting operation preserves the accuracy of larger sub-networks.",
qsTV*6p~$V,How does&nbsp;<b>Elastic Resolution&nbsp;</b>work in&nbsp;<b>once-for-all networks</b>?,"The resolution is <b>elastic throughout the whole training process</b>, which is implemented by sampling different image sizes for each batch of training data.",Once-for-All: Train One Network and Specialize it for Efficient Deployment<br>,https://arxiv.org/abs/1908.09791,,
