guid,question,answer,paper_title,paper_url,explanation,tags
LkzJPdb/!?,Draw the architecture of <b>DETR</b>.,"<img src=""paste-132b224ca8ac9dcdf056c2b7737a8ffb47c0e9c1.jpg""><img src=""paste-3a35091f801c1a07f766a113cc4bf98fe9d7225f.jpg"">",DETR: End-to-End Object Detection with Transformers,https://arxiv.org/abs/2005.12872,,
l{=hYS<?DL,"While <b>DETR </b>has a better AP than previous CNN based object detection algorithms, in which aspects is it worse than those models?",DETR performs <b>worse on small objects</b>.<br>DETR requires <b>extra-long training</b>.,DETR: End-to-End Object Detection with Transformers,https://arxiv.org/abs/2005.12872,,
mGlCpqb_P>,Why is&nbsp;<b>DETR </b>called an <b>End-to-End</b> detector?,"DETR predicts all objects at once, without an intermediate step such as non-maximal suppression.",DETR: End-to-End Object Detection with Transformers,https://arxiv.org/abs/2005.12872,This is done using a <b>set loss function</b> which performs <b>bipartite matching</b> between predicted and ground-truth objects.,
PJ#-8Ey8Y1,How does <b>DETR</b> match predictions with ground-truth?,"<div><div><div>DETR uses&nbsp;<b>bipartite matching</b> between predicted and ground truth objects.</div></div>Let us denote by \(y\) the ground truth set of objects, and \(\hat{y} = \{\hat{y}_i\}_{i=1}^{N}\) the set of \(N\) predictions.</div><div><i>Assuming \(N\) is larger than the number of objects in the image</i>,</div><div>we consider \(y\) also as a set of size \(N\) padded with \(\emptyset\) (no object).</div><div>To find a bipartite matching between these two sets we search for a permutation of \(N\) elements \(\sigma \in \Sigma_N\) with the lowest cost:</div><div><div>\[\hat{\sigma} = \text{argmin}_{\sigma\in\Sigma_N} \sum_{i}^{N} L_{match}(y_i, \hat{y}_{\sigma(i)}),\]</div></div><div>where \(\cal{L}_{match}(y_i, \hat{y}_{\sigma(i)})\) is a pair-wise matching cost between ground truth \(y_i\) and a prediction with index \(\sigma(i)\). <br>This optimal assignment is computed efficiently with the <b>Hungarian algorithm</b>.<br><div><div>The matching cost takes into account both the class prediction and the similarity of predicted and ground truth boxes. Each element \(i\) of the ground truth set can be seen as a \(y_i = (c_i, b_i)\) where \(c_i\) is the target class label (which may be \(\emptyset\)) and \(b_i \in [0, 1]^4\) is a vector that defines ground truth box center coordinates and its height and width relative to the image size. For the prediction with index \(\sigma(i)\) we define probability of class \(c_i\) as \(\hat{p}_{\sigma(i)}(c_i)\) and the predicted box as \(\hat{b}_{\sigma(i)}\). With these notations we define</div><div>\(\cal{L}_{match}(y_i, \hat{y}_{\sigma(i)})\) as \(-\mathbb{1}_{\{c_i\neq\emptyset\}}\hat{p}_{\sigma(i)}(c_i) + \mathbb{1}_{\{c_i\neq\emptyset\}} \cal{L}_{box}(b_{i}, \hat{b}_{\sigma(i)})\).</div></div></div>",DETR: End-to-End Object Detection with Transformers,https://arxiv.org/abs/2005.12872,,
Ok6w68_sEk,Which <b>loss function</b> is used in <b>DETR</b>?,"The <b>Hungarian loss</b>.&nbsp;<br><div><div>Which is a linear combination of a <u>negative log-likelihood </u>for class prediction and a box loss:</div><div>\[\cal{L}_{Hungarian}(y, \hat{y}) = \sum_{i=1}^N \left[-\log \hat{p}_{\hat{\sigma}(i)}(c_{i}) + \mathbb{1}_{\{c_i\neq\emptyset\}} \cal{L}_{box}(b_{i}, \hat{b}_{\hat{\sigma}}(i))\right]\]<br><div><div>where&nbsp;<br>\[\cal{L}_{box}(b_{i}, \hat{b}_{\hat{\sigma}}(i)) = \lambda_{\rm iou}\cal{L}_{iou}(b_{i}, \hat{b}_{\sigma(i)}) + \lambda_{\rm L1}||b_{i}- \hat{b}_{\sigma(i)}||_1 \]<br>with \(\cal{L}_{iou}\) the <u>generalized IoU loss</u>&nbsp;and \(\hat{\sigma}\) the optimal assignment computed with the <u>Hungarian algorithm</u>.</div></div></div></div>",DETR: End-to-End Object Detection with Transformers,https://arxiv.org/abs/2005.12872,,
hqkYHP-fZ&,How does <b>DETR </b>produce N predictions.,"The predictions come from the <b>transformer decoder</b>.<br><br><div><div>The decoder follows the standard architecture of the transformer, transforming \(N\) embeddings of size \(d\) using multi-headed self- and encoder-decoder attention mechanisms. The difference with the original transformer is that our model decodes the \(N\) objects in <i>parallel </i>at each decoder layer.</div><div>Since the decoder is also permutation-invariant, the \(N\) input embeddings must be different to produce different results. These input embeddings are <i>learnt positional encodings</i> that we refer to as <b>object queries</b>, and similarly to the encoder, we add them to the input of each attention layer.&nbsp;</div><div>The \(N\) object queries are transformed into an output embedding by the decoder. They are then <i>independently </i>decoded into box coordinates and class labels by a feed forward network, resulting \(N\) final predictions. Using self- and encoder-decoder attention over these embeddings, the model globally reasons about all objects together using pair-wise relations between them, while being able to use the whole image as context.&nbsp;</div></div>",DETR: End-to-End Object Detection with Transformers,https://arxiv.org/abs/2005.12872,,
oEa*`6IEbt,How many FLOPS and parameters does the <b>DETR </b>model have? And how accurate is it on COCO?,<b>86G FLOPS</b> and <b>41M</b> parameters with <b>AP 42.0</b> on COCO.,DETR: End-to-End Object Detection with Transformers,https://arxiv.org/abs/2005.12872,,
