guid,question,answer,paper_title,paper_url,explanation,tags
"g<%,dt5NO4",What is the main <b>difference </b>between the original slimmable networks paper and the <b>universally slimmable networks</b> paper?,"The originally slimmable networks could only be slimmed to predefined widths (e.g. [0.25, 0.5, 0.75, 1.0]), the universally slimmable networks (US-Nets)<b> extend this to networks of arbitrary width</b>.",Universally Slimmable Networks and Improved Training Techniques<br>,https://arxiv.org/abs/1903.05134,"<img src=""paste-fbc9e5d9a20d571719f5aac761c3ce8c8ae820a6.jpg"">",
H%(Et&$@p-,"One the main issues and reason to work with predefined widths in Slimmable Networks was batch normalization, how is that solved in Universally Slimmable networks?",The batch normalization only caused issues during testing. In universally trainable networks they argue that you can just compute the batch norm statistics of an arbitrary width with a large minibatch before you start using it.,Universally Slimmable Networks and Improved Training Techniques<br>,https://arxiv.org/abs/1903.05134,,
s4/eG=IKsc,What are the main new techniques in Universally Slimmable networks?,"<b>The sandwich rule:</b><div>Instead of running the model at fixed widths as in Slimmable Networks they randomly sample \(n-2\) widths in the range \([0.25, 1.0] \times\) and train the model with these widths along with the smallest and largest width.</div><div><b>Inplace distillation:</b></div><div>For the largest model the ground truth is used as labels but for all other models the predicted labels of this largest model is used as training label.</div>",Universally Slimmable Networks and Improved Training Techniques<br>,https://arxiv.org/abs/1903.05134,,
