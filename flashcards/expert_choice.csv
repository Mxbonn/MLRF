guid,question,answer,paper_title,paper_url,explanation,tags
LXL>a6)~VV,What new approach to MoE does <b>Mixture-of-Experts with Expert Choice Routing</b>&nbsp;introduce?,"Expert Choice introduces a Mixture-of-Experts where<b> instead of letting tokens select the top-\(k\) experts, the experts select the top-\(k\) tokens</b>.<br><img src=""paste-52b754c81bc1dec5d0bb509a4a62f1861f4611cb.jpg"">",Mixture-of-Experts with Expert Choice Routing,https://arxiv.org/abs/2202.09368,,
dU~J%|!nBp,What advantage does <b>Expert Choice</b> bring to <b>MoE</b>?,The Expert Choice approach achieves <b>perfect load balancing</b> without additional loss functions.,Mixture-of-Experts with Expert Choice Routing,https://arxiv.org/abs/2202.09368,,
bGh|>Nu7dA,How it&nbsp;<b>routing</b>&nbsp;done in&nbsp;<b>Expert Choice MoE</b>?,"Given input matrix \(X \in \mathbb{R}^{n \times d}\) and \(e\) experts, the&nbsp;<i>token-to-expert affinity scores</i> are computed as:<br>\[S = \operatorname{Softmax}(X \cdot W_g), \text{where } W_g \in \mathbb{R}^{d \times e}, S \in \mathbb{R}^{n \times e}\]A <b>token-to-expert assignment</b> is represented by three matrices: \(I, G \in \mathbb{R}^{e\times k}\) and \(P \in \mathbb{R}^{e \times k \times n}\).&nbsp;<br>\(I[i,j]\) indicates which token is the \(j\)-th selected token of the \(i\)-th expert. \(P\) is the one-hot version of \(I\) and \(G\) is the gating matrix that stores the routing weights of the selected tokens.<br>\[G, I = \operatorname{TopK}(S^\top, k)\quad P = \operatorname{one-hot}(I)\]<br><br>",Mixture-of-Experts with Expert Choice Routing,https://arxiv.org/abs/2202.09368,,
xhwmQ>z@WJ,How is <b>\(k\)</b> set in <b>Expert Choice MoE</b>?,"\(k\) is set as <b>\(k = \frac{n \times c}{e}\)</b>, where \(n\) is the total number ot tokens in the batch, \(c\) the capacity factor, and \(e\) the number of experts.",Mixture-of-Experts with Expert Choice Routing,https://arxiv.org/abs/2202.09368,"The paper used \(c = 2\) in most experiments, but showed that \(c = 1\) still outperformed the Switch Transformer. Even \(c = 0.5\) outperformed top-1 token choice gating as done in the Switch Transformer.",
MbN)];WD$X,Should the <i>number of experts per token be limited</i>/regularized in <b>Expert Choice Moe</b>?,<b>No.</b>&nbsp;<br>The authors tried regularizing this but found it to decrease the performance.,Mixture-of-Experts with Expert Choice Routing,https://arxiv.org/abs/2202.09368,,
xZW/;DQboR,What are the <b>limitations of Expert Choice MoE</b>?,It shares the limitation of most MoE methods: it requires <b>large batch sizes</b>.<br>AND it has a limitation unique to the expert choice approach: it <b>does not work for auto-regressive text generation</b> as the implementation needs to know the past and&nbsp;<i>future</i>&nbsp;tokens to perform the top-\(k\) selection.,Mixture-of-Experts with Expert Choice Routing,https://arxiv.org/abs/2202.09368,,
