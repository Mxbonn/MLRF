guid,question,answer,paper_title,paper_url,explanation,tags
qC!w~g2u.3,Give the pseudocode for&nbsp;<b>Deep Q-learning with Experience Replay</b>.,"<div style=""text-align: left;"">Initialize replay memory&nbsp;\(D\) to capacity \(N\)</div><div style=""text-align: left;"">Initialize action-value function \(Q\) with random weights \(\theta\)<br>or episode = 1, \(M\) do<br>&nbsp; &nbsp; Initialize sequence \(s_1 = \{x_1\}\) and preprocessed sequence&nbsp;&nbsp;\(\phi_1 = \phi(s_1)\)<br>&nbsp; &nbsp; for \(t = 1, T\) do<br>&nbsp; &nbsp; &nbsp; &nbsp; With probability \(\epsilon\) select a random action \(a_t\)<br>&nbsp; &nbsp; &nbsp; &nbsp; Otherwise select \(a_t = \operatorname{argmax}_a Q(\phi(s_t),a;\theta)\)<br>&nbsp; &nbsp; &nbsp; &nbsp; Execute action \(a_t\) in emulator and observe reward \(r_t\) and image \(x_{t+1}\)<br>&nbsp; &nbsp; &nbsp; &nbsp; Set \(s_{t+1} = s_t, a_t, x_{t+1}\) and preprocess \(\phi_{t+1}=\phi(s_{t+1})\)<br>&nbsp; &nbsp; &nbsp; &nbsp; Store transition \((\phi_t, a_t, r_t, \phi_{t+1})\) in \(D\)<br>&nbsp; &nbsp; &nbsp; &nbsp; Sample random minibatch of transitions&nbsp;\((\phi_j, a_j, r_j, \phi_{j+1})\) from \(D\)<br>&nbsp; &nbsp; &nbsp; &nbsp; set \(y_j = \begin{cases} r_j &amp; \text{if episode terminates at step } j+1 \\ r_j + \gamma \operatorname{max}_{a'} Q (\phi_{j+1},a'; \theta) &amp; \text{otherwise}\end{cases}\)<br>&nbsp; &nbsp; &nbsp; &nbsp; Perform a gradient descent step on \((y_j - Q(\phi_j, a_j;\theta))^2\) with respect to the network parameters \(\theta\)<br>&nbsp; &nbsp; &nbsp; &nbsp; Every \(C\) steps reset \(\hat{Q} = Q \)<br>&nbsp; &nbsp; End for<br>End for</div><br>",Playing Atari with Deep Reinforcement Learning,https://arxiv.org/abs/1312.5602,,
Pld07Hd/xA,Why do we create&nbsp;<b>a replay memory in Deep Q-learning</b>?,"Experience replay in Deep Q-Learning has two functions:<br>1.&nbsp;<b>Make more efficient use of the experiences during training</b>. Usually, in online RL, the agent interacts in the environment, gets the experiences (state, action, reward, and next state), learns from them (updates the neural network), and discards them. This is not efficient.<br><br>Experience replay helps&nbsp;<b>using the experiences of the training more efficiently</b>. We use a replay buffer that saves experience samples&nbsp;<b>that we can reuse during the training</b>.<br>This allows the agent to&nbsp;<b>learn from the same experiences multiple times</b>.<br><br>2.&nbsp;<b>Avoid forgetting previous experiences and reduce the correlation between experiences.</b><br>Experience replay also has other benefits. By randomly sampling the experiences, we remove correlation in the observation sequences and avoid&nbsp;<b>action values from oscillating or diverging catastrophically</b>.",Playing Atari with Deep Reinforcement Learning,https://arxiv.org/abs/1312.5602,See also: https://huggingface.co/deep-rl-course/unit3/deep-q-algorithm,
