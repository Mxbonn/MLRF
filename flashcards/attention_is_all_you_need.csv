guid,question,answer,paper_title,paper_url,explanation,tags
xY)Za+P~<{,Draw the general architecture of the <b>Transformer </b>model.,"<img src=""transformer.png"">",Attention Is All You Need,https://arxiv.org/abs/1706.03762,,
K{>D2en+W7,"What is an&nbsp;<b>attention&nbsp;</b>function&nbsp;according to the ""Attention is All you Need"" paper?","An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the&nbsp;query with the corresponding key.",Attention Is All You Need,https://arxiv.org/abs/1706.03762,"<span style=""color: rgb(36, 39, 41);"">The key/value/query concepts come from retrieval systems.&nbsp;</span><div><span style=""color: rgb(36, 39, 41);"">The attention operation turns out can be thought of as a retrieval process as well, so the key/value/query concepts also apply here.</span><span style=""color: rgb(36, 39, 41);""><br></span></div>",
AMS}6A(/Ya,What is <b>Scaled Dot-Product Attention</b>?,"<img src=""paste-0c5d0fec9e6c9309e4b1c0900d2034fb0b6ad0b2.jpg""><div><span style=""color: rgb(51, 51, 51);"">The input consists of queries and keys of dimension \(d_k\)</span><span style=""color: rgb(51, 51, 51);"">, and values of dimension&nbsp;\(d_v\)</span><span style=""color: rgb(51, 51, 51);"">. We compute the dot products of the query with all keys, divide each by&nbsp;\(\sqrt{d_k}\)</span><span style=""color: rgb(51, 51, 51);"">, and apply a softmax function to obtain the weights on the values.</span><br></div><div><span style=""color: rgb(51, 51, 51);""><br></span></div><div><span style=""color: rgb(51, 51, 51);"">In practice, attention function is computed on a set of queries simultaneously, packed together into a matrix \(Q\)</span><span style=""color: rgb(51, 51, 51);"">. The keys and values are also packed together into matrices&nbsp;\(K\)&nbsp;</span><span style=""color: rgb(51, 51, 51);"">and&nbsp;\(V\)</span><span style=""color: rgb(51, 51, 51);"">. Compute the matrix of outputs as:</span><span style=""color: rgb(51, 51, 51);""><br></span></div><div>\[\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V\]<br></div>",Attention Is All You Need,https://arxiv.org/abs/1706.03762,,
G^p4hP-Iwf,What is <b>Multi-Head Attention</b>?,"<img src=""paste-609856fcb71f586ff92ed2bcb512e168bd03216b.jpg""><div>Instead of performing a single attention function with \(d_\text{model}\)-dimensional keys, values and queries, multi-head attention <b>linearly project</b> the queries, keys and values <b>\(h\) times with different,</b> <b>learned </b>linear projections to \(d_k\), \(d_k\) and \(d_v\) dimensions, respectively. On each of these projected versions of queries, keys and values it then <b>performs the attention function in parallel</b>, yielding \(d_v\)-dimensional output values. These are <b>concatenated and once again projected</b>, resulting in the final values, as<br>depicted above.<br></div><div><br></div><div><span style=""color: rgb(51, 51, 51);"">Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</span><br></div><div>\[\mathrm{MultiHead}(Q, K, V) = \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O \\ \text{where}~\mathrm{head_i} = \mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)\]<br></div>",Attention Is All You Need,https://arxiv.org/abs/1706.03762,,
i?0i]=A;a1,"How do you get K, Q, V in Self-Attention?","<span style=""color: rgb(34, 34, 34);"">These vectors are created by multiplying the embedding/input by three matrices that are learned during the training process.</span>",Attention Is All You Need,https://arxiv.org/abs/1706.03762,"<img src=""paste-8fda89cac42155e8287f8d5fb8e7eeeb4b4829f0.jpg"">",
uuoc99;Q%6,What are the <b>three </b>different ways in which <b>multi-head attention</b> is used in Transformers?,"1)&nbsp;The <b>encoder </b>contains <b>self-attention</b> layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.<div><br></div><div>2)&nbsp;<span style=""color: rgb(51, 51, 51);"">Similarly, <b>self-attention layers in the decoder </b>allow each position in the decoder to <b>attend to all positions in the decoder up to and including that position</b>. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot- product attention by <b>masking </b>out (setting to \(- \infty\)</span><span style=""color: rgb(51, 51, 51);"">) all values in the input of the softmax which correspond to illegal connections.</span></div><div><span style=""color: rgb(51, 51, 51);""><br></span></div><div><span style=""color: rgb(51, 51, 51);"">3)&nbsp;</span>In ""encoder-decoder attention"" layers, the <b>queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.</b> This allows every position in the decoder to attend over all positions in the input sequence</div>",Attention Is All You Need,https://arxiv.org/abs/1706.03762,,
"t~va,Cm{%k",How are the&nbsp;<b>feed forward layers</b>&nbsp;used in the&nbsp;<b>Transformer&nbsp;</b>model?,"<span style=""color: rgb(51, 51, 51);"">In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.&nbsp;</span>For this reason it's often called a&nbsp;<b>Position-wise Feed Forward layer</b>.<span style=""color: rgb(51, 51, 51);"">&nbsp;</span><div><span style=""color: rgb(51, 51, 51);"">This layer consists of two linear transformations with a ReLU activation in between.</span><div>\[\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2\]<br></div><div><br></div></div>",Attention Is All You Need,https://arxiv.org/abs/1706.03762,"<span style=""color: rgb(51, 51, 51);"">Another way of describing this is as two convolutions with kernel size 1.</span><div><img src=""paste-69e97811bf70df5bb610448816acdffa57d8b0e7.jpg""><span style=""color: rgb(51, 51, 51);""><br></span></div>",
QIn]Qcxf9G,Give an overview of how the <b>transformer</b>&nbsp;model works during inference?,"<div>The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:</div><img src=""transformer_decoding_1.gif""><br><div><span style=""color: rgb(34, 34, 34);"">The following steps repeat the process until a special&nbsp;</span>symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.<br></div><div><img src=""transformer_decoding_2.gif""><br></div>",Attention Is All You Need,https://arxiv.org/abs/1706.03762,,
Ab4%XN&xay,What are <b>positional encodings</b> in Transformers and why are they used?,"<span style=""color: rgb(51, 51, 51);"">Since the model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, <b>we must inject some information about the relative or absolute position of the tokens in the sequence</b>. To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension \(d_\text{model}\)&nbsp;</span><span style=""color: rgb(51, 51, 51);"">as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed</span><span style=""color: rgb(51, 51, 51);"">.</span><div><span style=""color: rgb(51, 51, 51);""><br></span></div><div><div>In this work, they use sine and cosine functions of different frequencies: \(PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\text{model}}})\)</div><div>where&nbsp;\(pos\) is the position and&nbsp;\(i\) is the dimension.&nbsp;</div></div>",Attention Is All You Need,https://arxiv.org/abs/1706.03762,"<img src=""attention-is-all-you-need-positional-encoding.png"">",
Qs*Ry_Cp(u,Why are masks used in Transformers?,"<div>The purpose of masking is that you prevent the decoder state from attending to positions that correspond to tokens ""in the future"", i.e., those that will not be known at the inference time, because they will not have been generated yet.</div><div>At inference time, it is no longer a problem because there are no tokens from the future, there have not been generated yet.</div>",Attention Is All You Need,https://arxiv.org/abs/1706.03762,"See also&nbsp;<a href=""https://datascience.stackexchange.com/questions/80826/transformer-masking-during-training-or-inference"">https://datascience.stackexchange.com/questions/80826/transformer-masking-during-training-or-inference</a>",
