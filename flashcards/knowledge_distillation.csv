guid,question,answer,paper_title,paper_url,explanation,tags
i=?(Pj2ajz,What is <b>Knowledge Distillation</b>?,"Knowledge distillation is&nbsp;<b>model compression technique in which a small model is trained to mimic a pre-trained, larger model</b>&nbsp;(or ensemble of models).<br>This training setting is sometimes referred to as&nbsp;<b>teacher-student</b>, where the large model is the teacher and the small model is the student.",Distilling the Knowledge in a Neural Network,https://arxiv.org/abs/1503.02531,,
f`H8>:n(~U,How do you train a network with <b>knowledge distillation</b>?,"In distillation, knowledge is transferred from the teacher model to the student by&nbsp;<b>using the class probabilities produced by the teacher as&nbsp;soft targets&nbsp;for the student</b>.<br><br>However, in many cases, this probability distribution has the correct class at a very high probability, with all other class probabilities very close to 0. In distillation the&nbsp;<b>temperature of the final softmax&nbsp;is&nbsp;raised</b>&nbsp;until the teacher produces a suitable soft set of targets.<br><br>The probability \(p_i\)&nbsp;of class&nbsp;\(i\)&nbsp;is calculated from the logits&nbsp;\(z_i\)&nbsp;as:<br>\[p_i = \frac{exp\left(\frac{z_i}{T}\right)}{\sum_{j} \exp\left(\frac{z_j}{T}\right)}\]<br>where \(T\) is the temperature.<br><br>In the original paper by Hinton et al. the loss of the&nbsp;soft labels&nbsp;is combined with the loss of the&nbsp;hard labels/targets.<br><br>The overall loss function, incorporating both distillation and student losses is calculcated as:<br>\[\mathcal{L}(x;W) = \alpha * \mathcal{H}(y, \sigma(z_s; T=1)) + \beta * \mathcal{H}(\sigma(z_t; T=\tau), \sigma(z_s, T=\tau))\]<br>where \(x\) is the input, \(W\) are the student model parameters, \(y\) is the ground truth label, \(\mathcal{H}\) is the cross-entropy loss function, \(\sigma\) is the softmax function parameterized by the temperature \(T\), and \(\alpha\)&nbsp; and \(\beta\) are coefficients. \(z_s\) and \(z_t\) are the logits of the student and teacher respectively.<br><br>In general \(\tau\), \(\alpha\) and \(\beta\) are hyper parameters.<br><img src=""knowledge_distillation.png"">",Distilling the Knowledge in a Neural Network,https://arxiv.org/abs/1503.02531,"See also:&nbsp;<a href=""https://intellabs.github.io/distiller/knowledge_distillation.html"">https://intellabs.github.io/distiller/knowledge_distillation.html</a>",
oaK-3]v+K&,What is the formula for the <b>softmax function with temperature scaling</b>?,\[\sigma(z_i;T) = \frac{exp\left(\frac{z_i}{T}\right)}{\sum_{j} \exp\left(\frac{z_j}{T}\right)}\],Distilling the Knowledge in a Neural Network,https://arxiv.org/abs/1503.02531,,
B8ko89?|YR,Why do you need to use a softmax with a <b>raised temperature</b>&nbsp;when doing knowledge distillation?,"Using a normal softmax, the correct class probability has often a very high probability, with all other class probability close to 0.&nbsp;<span style=""color: rgb(64, 64, 64); background-color: rgb(252, 252, 252);"">As such, it doesn't provide much information beyond the ground truth labels already provided in the dataset.</span>",Distilling the Knowledge in a Neural Network,https://arxiv.org/abs/1503.02531,<b>Note:</b>&nbsp;During <b>inference</b> the temperature is set back to <b>1</b>.,
