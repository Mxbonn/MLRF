guid,question,answer,paper_title,paper_url,explanation,tags
Lx=A<Dp<ae,Which issues with (mip)-NeRF does mip-NeRF 360 address?,1. (mip)-NeRF struggles with <b>unbounded scenes</b>. mip-NeRF 360 reparameterizes the scenes such that they lay in a bounded space.<br>2. (mip)-NeRF training requires<b> many iterations</b> and is <b>expensive</b>. mip-NeRF 360 introduces online distillation<br>3. (mip)-NeRF has <b>artefacts</b> in large scenes due to <b>ambiguity</b> due to few samples per observation. mip-NeRF 360 adds specific regularization to fix this.,Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields,https://arxiv.org/abs/2111.12077,,
zFA3!hgImQ,What is the <b>problem with unbounded scenes in mip-NeRF</b> that <i>mip-NeRF 360</i> fixes?,"mip-NeRF requires <b>bounded rays</b>, as we cannot parameterize an infinite sized ray.&nbsp;<br><img src=""paste-ad974849cbca0dca35d8b7a38467aa7f1d3a2962.jpg"">",Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields,https://arxiv.org/abs/2111.12077,,
cP`7hMtR[+,How does <b>mip-NeRF 360 reparameterize the multivariate gaussians</b> that are used to parameterize rays in the original mip-NeRF?,"To do this, first let us define \(f(\mathbf{x})\) as some smooth <b>coordinate transformation</b> that maps from \(\mathbb{R}^n \rightarrow \mathbb{R}^n\) (in this case, \(n=3$\). We can compute the <b>linear approximation of this</b> function:<br>\[f(\mathbf{x}) \approx f(\mathbf{\boldsymbol{\mu}}) + \mathbf{J}_{f}(\mathbf{\boldsymbol{\mu}})(\mathbf{x} - \mathbf{\boldsymbol{\mu}})\]<br>Where \(\mathbf{J}_{f}(\mathbf{\boldsymbol{\mu}})\) is the Jacobian of \(f\) at \(\boldsymbol{\mu}\). With this, we can apply \(f\) to \((\boldsymbol{\mu}, \boldsymbol{\Sigma})\) as follows:<br>\[f(\boldsymbol{\mu}, \boldsymbol{\Sigma}) = \left( f(\boldsymbol{\mu}), \, \mathbf{J}_{f}(\mathbf{\boldsymbol{\mu}}) \boldsymbol{\Sigma} \mathbf{J}_{f}(\mathbf{\boldsymbol{\mu}})^\mathrm{T} \right)\]<br><i>This is functionally equivalent to the classic Extended Kalman filter</i>, where \(f\) is the state transition model.<br>Our choice for \(f\) is the following contraction:<br><b>\[\operatorname{contract}(\mathbf{x}) = \begin{cases}
\mathbf{x} &amp; \|{\mathbf{x}}\| \leq 1\\
\left(2 - \frac{1}{\|{\mathbf{x}}\|}\right)\left(\frac{\mathbf{x}}{\|{\mathbf{x}}\|}\right) &amp; \|{\mathbf{x}}\| &gt; 1
\end{cases}\]</b><img src=""4-Figure2-1.png""><br>Instead of using mip-NeRF's <i>IPE</i> features in Euclidean space we use similar features&nbsp; in this contracted space: <b>\(\gamma( \operatorname{contract}(\boldsymbol{\mu}, \boldsymbol{Sigma}))\)</b>.",Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields,https://arxiv.org/abs/2111.12077,,
zVDT_I?E?r,How are the rays sampled in mip-NeRF 360?,They are sampled&nbsp;<b>linearly&nbsp;</b>in&nbsp;<i>inverse depth</i>&nbsp;(<b>disparity</b>).,Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields,https://arxiv.org/abs/2111.12077,More details in the paper.,
FeTLzd`iCx,"Given the following schematic of the mip-NeRF training pipeline, <br><img src=""crop0.png""><br><b>give the schematic of the mip-NeRF 360 pipeline:</b>","<img src=""crop1.png"">",Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields,https://arxiv.org/abs/2111.12077,"While <b><i>mip-NeRF </i>uses one multi-scale MLP</b> that is repeatedly queried (only two repetitions shown here) for weights that are resampled into intervals for the next stage, and supervises the renderings produced at all scales.&nbsp;<b><i>mip-NeRF 360</i> use a <i>proposal</i> MLP</b> that emits weights (but not color) that are resampled, and in the final stage we use a <b><i>NeRF MLP</i> to produce weights and colors</b> that result in the rendered image, which we supervise. The proposal MLP is trained to produce proposal weights \(\mathbf{\hat{w}}\) that are consistent with the NeRF MLPâ€™s \(\mathbf{w}\) output. By using a small proposal MLP and a large NeRF MLP we obtain a combined model with a high capacity that is still tractable to train",
Bq6gx*p3v>,<b>Which loss function</b> is used to train the <b>proposal MLP</b> used for the <b>online distillation</b> in <i>mip-NeRF 360</i>?,"The online distillation requires <b>a loss function that encourages the histograms emitted by the proposal MLP </b>\((\hat{\mathbf{t}}, \hat{\mathbf{w}})\)<b> and the NeRF MLP </b>\((\mathbf{t}, \mathbf{w})\)<b> to be consistent</b>.<br>If the two histograms are consistent with each other, then it must hold that \(w_i \leq \operatorname{bound}\left( \hat{\mathbf{t}}, \hat{\mathbf{w}}, T_i \right)\)&nbsp; for all intervals \((T_i, w_i)\) in \((\mathbf{t}, \mathbf{w})\)&nbsp;with&nbsp;<br>\[\operatorname{bound}\left( \hat{\mathbf{t}}, \hat{\mathbf{w}}, T \right) = \sum_{j: \, T \cap \hat{T}_j \neq \varnothing} \hat w_j\]<img src=""paste-dcccd052788401037b4c014ee94d1a642c31a064.jpg""><br><b>The proposal loss penalizes any surplus histogram mass that violates this inequality and exceeds this bound</b>:<br>\[\mathcal{L}_{\text{prop}}\left(\mathbf{t}, \mathbf{w}, \hat{\mathbf{t}}, \hat{\mathbf{w}} \right)\!=\! \sum_{i}\frac{1}{w_{i}}\max\left( 0, w_{i} - \operatorname{bound}\left( \hat{\mathbf{t}}, \hat{\mathbf{w}}, T_i \right) \right)^{2}\]<br><img src=""paste-d819e0dd4c06b9f2411fc2e9bac02634b41af0ba.jpg""><br>We impose this loss between the NeRF histogram \((\mathbf{t}, \mathbf{w})\) and all proposal histograms \((\hat{\mathbf{t}}^k, \hat{\mathbf{w}}^k)\). <b>A stop-gradient is placed on the NeRF MLP</b>'s outputs \(\mathbf{t}\) and \(\mathbf{w}\) when computing \(\mathcal{L}_{\text{prop}}\)&nbsp; so that the <b>NeRF MLP <i>leads</i> and the proposal MLP </b><i><b>follows</b>,</i>&nbsp;otherwise the NeRF may be encouraged to produce a worse reconstruction of the scene so as to make the proposal MLP's job less difficult.<br>",Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields,https://arxiv.org/abs/2111.12077,,
hO*U#B4x=u,What are the <b>artifacts</b> that <i>mip-NeRF 360</i> tries to avoid with its additional regularization loss?,"<b>Floaters</b>: the small disconnected regions of volumetrically dense space that look like blurry clouds when viewed from another angle.<br><b>Background collapse</b>: is the phenomenon in which distant surfaces are incorreclty modeled as smi-transparent clouds of dense content close to the camera.<br><img src=""360_ab_nodistortion_noise1_bicycle_002_rgb_comp.jpg""><br><img src=""360_ab_nodistortion_noise1_bicycle_002_dist.png"">",Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields,https://arxiv.org/abs/2111.12077,,
PgwFx+D-<,How does <b>mip-NeRF 360</b>&nbsp;eliminate floaters and background collapse artifacts?,"They add a regularizer that is defined in terms of the step function defined by the set of (normalized) ray distances \(\mathbf{s}\) and weights \(\mathbf{w}\) that parameterize each ray:<br>\[\mathcal{L}_{\text{dist}}(\mathbf{s}, \mathbf{w}) =\iint\limits_{-\infty }^{\,\,\,\infty }\mathbf{w}_\mathbf{s}(u)\mathbf{w}_\mathbf{s}(v) |u - v|\,d_{u}\,d_{v}\]<br>where \(\mathbf{w}_\mathbf{s}(u)\) is interpolation into the step function defined by \((\mathbf{s}, \mathbf{w})\) at \(u\): <br>\(\mathbf{w}_\mathbf{s}(u) = \sum_i w_i \mathbb{1}_{[s, s_{i+1})}(u)\).<br>This loss is the integral of the distances between all pairs of points along this 1D step function, scaled by the weight \(w\) assigned to each point by the NeRF MLP. We refer to this as <i>distortion</i>.<br>This loss encourages each ray to be as compact as possible by 1) <b>minimizing the width of each interval</b>, 2) <b>pulling distant intervals towards each other</b>, 3) <b>consolidating weight into a single interval or a small number of nearby intervals</b>, and 4) <b>driving all weights towards zero</b> when possible (such as when the entire ray is unoccupied).<br><img src=""paste-3bc9cb36f9de89c137530159ade1966c1e83c16e.jpg""><br>",Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields,https://arxiv.org/abs/2111.12077,"Though \(\mathcal{L}_{\text{dist}}\) is straightforward to define, it is non-trivial to compute.<br>But because \(\mathbf{w}_\mathbf{s}(\cdot)\) has a constant value within each interval we can rewrite it as:<br>\[\begin{align}
\mathcal{L}_{\text{dist}}(\mathbf{s}, \mathbf{w}) &amp;=
\sum_{i,j} w_{i} w_{j} \left| \frac{s_{i} +s_{i+1}}{2} - \frac{s_{j} + s_{j+1}}{2} \right| \nonumber \\
&amp;+ \frac{1}{3}\sum _{i} w_{i}^{2}( s_{i+1} - s_{i}) 
\end{align}\]",
