guid,question,answer,paper_title,paper_url,explanation,tags
EGriZ7W}<L,Give an overview of the <b>Switch Transformer</b>.,"The Switch Transformer is <b>transformer based model</b> that incorporates a <b>Mixture of Experts (MoE)</b> in the feed forward layers.<br><img src=""switch-transformer.png""><br>The Switch Transformer encoder block replaces the dense feed forward network FFN with a sparse Switch FFN layer.<br>The layer operates independently on the tokens in the sequence.&nbsp;<br>The Switch FFN layer returns the output of the selected FFN multiplied by the router gate value (dotted-line).",Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,https://arxiv.org/abs/2101.03961,,
DKqRyM}2v^,How is <b>routing/gating</b> done in the <b>Switch Transformer MoE</b>?,The Switch Transformer uses the <b>same gating network as introduced in the original MoE paper</b> of Shazeer et al. 2017 <b>but it only routes to 1 expert</b>.<br>So \(k = 1\).<br>So for the Switch FFN:&nbsp;\(y = p_i(x)E_i(x)\) where \(i = \operatorname{argmax}(p(x))\).,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,https://arxiv.org/abs/2101.03961,,
z:b0N9_?<>,How is the <b>load/importance balanced</b> in the <b>Switch Transformer</b>?,"Just like the MoE paper from Shazeer et al. 2017 they use an <b>auxiliarly loss</b>.<br>The auxiliary loss in the Switch Transformer combines load-balancing and importance-weighting (this were two seperate losses in Shazeer et al.).<br><br>Given \(N\) experts indexed by \(i\) and a batched input \(X\) with \(T\) tokens in total, the auxiliary loss is computed as the scaled dot-product between vectors \(f_i\)&nbsp;and \(P_i\),<br>\[\mathcal{L} = \alpha \cdot N \cdot \sum_{i=1}^N f_i \cdot P_i\]<br>where \(f_i\) is the fraction of tokens dispatch to expert \(i\),<br>\[f_i = \frac{1}{T}\sum_{x \in X}\mathbb{1}\{\operatorname{argmax}(p(x))=1\}\]and \(P_i\) is the fraction of the router probability allocated for expert \(i\),<br>\[P_i = \frac{1}{T}\sum_{x \in X} p_i(x)\]<br><b>This loss miminizes under a uniform distribution of \(P\) and \(f\)</b>.",Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,https://arxiv.org/abs/2101.03961,,
K;$PONZQ]),What is the&nbsp;<b>capacity factor</b>&nbsp;mentioned in <b>Switch Transformer</b>?,"For implementation/efficiency reasons, the tensor shapes of each expert is fixed ahead of time.<br>The <b>expert capacity,</b> which is the number of tokens each expert computes, is set by evenly dividing the number of tokens in the batch across the number of experts, and then further expanding it by&nbsp;<b>the capacity factor</b>.<br>\[\text{expert capacity }=(\frac{\text{tokens per batch}}{\text{number of experts}}) \times \text{ capacity factor}\]A capacity factor great than 1.0 creates additional buffer to accomodate for when tokens are not perfectly balanced across experts.",Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,https://arxiv.org/abs/2101.03961,"<img src=""1lmv9_cnCay-E83ztEty-rg.png"">",
d)GUC4B_*`,"What happens when <b>more tokens</b> are assigned to an expert <b>than</b> there is <b>capacity</b> for, in the <b>Switch Transformer</b>?","If too many tokens are routed to an expert, computation is skipped and the <b>token representation is passed directly to the next layer through the residual</b>.",Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,https://arxiv.org/abs/2101.03961,,
