guid,question,answer,paper_title,paper_url,explanation,tags
j=KS>j.:qz,Draw the architecture of the SEgmentation TRansformer (<b>SETR</b>).,"<img alt=""pipeline picture"" src="setr-1.png"><br><b>Schematic illustration of the proposed SEgmentation TRansformer (SETR)</b> (a). We first split an image into fixed-size patches,&nbsp;linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. To&nbsp;perform pixel-wise segmentation, we introduce different decoder designs: (b) progressive upsampling (resulting in a variant called SETR-PUP); and (c) multi-level feature aggregation (a variant called SETR-MLA).",Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers,https://arxiv.org/abs/2012.15840,,
Idf*}zUxm:,What is the&nbsp;<b>PUP</b>&nbsp;decoder in&nbsp;<b>SETR</b>?,"<b>Progressive UPsampling (PUP)</b>&nbsp;<br><img src=""figure1_b.png""><br>",Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers,https://arxiv.org/abs/2012.15840,"<b>PUP</b> uses a&nbsp;<i>progressive upsampling</i>&nbsp;strategy that alternates&nbsp;<b>conv layers</b>&nbsp;and&nbsp;<b>upsampling operations</b>&nbsp;(upsampling is restricted to \(2 \times). Hence a total of 4 operations are needed to reach the full resolution from patches with size \(\frac{H}{16} \frac{W}{16}\). When using this decoder, the models is denoted as&nbsp;<b>SETR-PUP</b>.",
wF#Qmb_j$A,What is the&nbsp;<b>MLA</b>&nbsp;decoder in&nbsp;<b>SETR</b>?,"<b>Multi-Level feature Aggregation (MLA)</b>&nbsp;<br><img src=""figure1_c.png""><br>",Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers,https://arxiv.org/abs/2012.15840,"MLA takes as input feature representations&nbsp;\(\{Z^m\} (m \in \{\frac{L_e}{M}, 2\frac{L_e}{M}, ..., M\frac{L_e}{M}\})\)&nbsp;from&nbsp;\(M\)&nbsp;layers uniformly distributed across the layers with step&nbsp;\(\frac{L_e}{M}\)&nbsp;to the decoder.&nbsp;\(M\)&nbsp;streams are then deployed, with each focusing on one specific selected layer.<br>A 3-layer network is applied with the feature channels halved at the first and third layers respectively, and the&nbsp;spatial resolution upscaled&nbsp;\(4 \times\)&nbsp;by bilinear operation after&nbsp;the third layer. To enhance the interactions across different streams, we introduce a top-down aggregation design&nbsp;via element-wise addition after the first layer. An additional&nbsp;\(3 \times 3\)&nbsp;conv is applied after the element-wise additioned feature. After the third layer, we obtain the fused feature from<br>all the streams via channel-wise concatenation which is then&nbsp;bilinearly upsampled&nbsp;\(4 \times\)&nbsp;to the full resolution. When using&nbsp;this decoder, we denote our model as&nbsp;<b>SETR-MLA</b>.",
