guid,question,answer,paper_title,paper_url,explanation,tags
j#meu>B_]O,Draw the Vision Transformer model.,"<img src=""paste-485c3c1399644cc6fb395eb2526d47c295309a23.jpg"">",An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://arxiv.org/abs/2010.11929,The model design <b>follows the original Transformer</b>&nbsp;(Vaswani et al) <b>as closely as possible.</b><br>An advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and their efficient implementations – can be used almost out of the box.,
c`[Avd[;J;,How the transformer adapted to make image classification predictions?,"Similar to BERT’s <b>[class] token</b>, we prepend a learnable embedding to the sequence of embedded patches (\(\mathbf{z}^0_0&nbsp;= \mathbf{x}_{\text{class}}\)), whose state at the output of the Transformer encoder (\(\mathbf{z}^0_L\)) serves as the<br>image representation \(\mathbf{y}\) (Eq. 4). Both during pre-training and fine-tuning, a classification head is attached to \(\mathbf{z}^0_L\). The classification head is implemented by a MLP with <i>one hidden layer at pre-training&nbsp;time and by a single linear layer at fine-tuning time</i>.",An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://arxiv.org/abs/2010.11929,,
y_qa>jL2hw,What type of <b>positional embeddings</b> are used in <b>Vision Transformers</b>?,Standard <b>learnable 1D position embeddings</b>.&nbsp;<br>Other embeddings were also tested but they did not observe significant performance gains from using more advanced 2D-aware position embeddings.&nbsp;,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://arxiv.org/abs/2010.11929,"<img src=""paste-467e1110fa7497af062c23d5451b1eb1dc4b8a7b.jpg""><br><b>Similarity of position embeddings of ViT-L/32.</b> Tiles show the cosine similarity between the position embedding of the patch with the indicated row and column and the position embeddings of all other&nbsp;patches.",
Atb=?wyzo&,What was the most crucial element to get <b>Transformers </b>to work well in <b>vision</b>?,"<b>Dataset size</b>.<br>When pre-trained on the smallest dataset, ImageNet, ViTmodels underperform compared to Resnet models. With ImageNet-21k pre-training, their performances are similar. Only<br>with JFT-300M, do we see the full benefit of the transformer models.",An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://arxiv.org/abs/2010.11929,"<img src=""paste-3f5c4508fb5c14ddbad8255a8c9079733ff442d6.jpg"">",
prs<y5u{2J,What is a possible explanation to why vision transformers perform worse on small datasets but better on very large datasets?,"The intuition is that the convolutional inductive bias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from&nbsp;data is sufficient, even beneficial.",An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://arxiv.org/abs/2010.11929,,
"u},2CT#fB@",What changes do you need to make to fine-tune <b>ViT</b>?,"Remove the pre-trained prediction head and attach a zero-initialized feedforward layer (so one hidden layer instead of 2). <br>It is often beneficial to <b>fine-tune at higher resolution than pre-training</b>. <br>When feeding images of higher resolution, we keep the patch size the same, which results in a larger effective sequence length. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints), <i>however, the pre-trained position embeddings may no longer be meaningful. We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image.</i>",An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://arxiv.org/abs/2010.11929,,
z5d{51EQ=v,How many FLOPS and paramers does the <b>ViT&nbsp;</b>model have? And how accurate is it on ImageNet?,<b>ViT-B/16</b> has <b>86M Params 33G FLOPS</b> and an ImageNet accuracy of <b>85.43</b>.<br><b>ViT-L/16 </b>has <b>304M Params 117G FLOPS</b>&nbsp;and an ImageNet accuracy of&nbsp;<b>85.63</b>.,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://arxiv.org/abs/2010.11929,The reported accuracies come from the official models when pre-trained on ImageNEt-21k.,
b0qv2#Js6r,"How long does it take to pretrain a ViT-L/16 model on the public ImageNet-21k dataset, using standard cloud TPUv3 machines?",<b>240 TPUv3-core-days.</b><br>Or 1 month when using 8 cores.,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://arxiv.org/abs/2010.11929,"While this does require a lot of compute, <b>Vision Transformers are more efficient to pretrain than ResNet based architectures.<br></b><img src=""paste-8b6b82ed26192e3ab97c597a9e7e113f72ae10c9.jpg""><b><br></b>",
