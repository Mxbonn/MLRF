guid,question,answer,paper_title,paper_url,explanation,tags
QHc.i6H]HT,What is&nbsp;<b>Proximal Policy Optimization</b> (<b>PPO</b>)?,"<b>Proximal Policy Optimization (PPO)</b>, is an algorithm that<b>&nbsp;improves an agent’s training stability by avoiding too large policy updates</b>. To do that, it uses a ratio that indicates the difference between the current and old policy and clips this ratio from a specific range&nbsp;\([1 - \epsilon, 1 + \epsilon]\).",Proximal Policy Optimization Algorithms,https://arxiv.org/abs/1707.06347,,
e=(l+thD^N,Give the objective function used in&nbsp;<b>Proximal Policy Optimization</b>&nbsp;(<b>PPO</b>).,"<div>First, let’s denote the probability ratio between old and new policies as:</div><div>\[r_t(\theta) = \frac{\pi_\theta(a_t \vert s_t)}{\pi_{\theta_\text{old}}(a_t \vert s_t)}\]<span style=""color: rgb(31, 31, 31); background-color: rgb(255, 255, 255);"">Then, the objective function of PPO becomes:</span></div><div><span style=""color: rgb(31, 31, 31); background-color: rgb(255, 255, 255);"">\[L^\text{PPO} (\theta) = \mathbb{E}_t [ r_t(\theta) \hat{A}_t]\]</span><b>PPO</b> imposes the constraint by forcing \(r_t(\theta)\) to stay within a small interval around 1, precisely \([1 - \epsilon, 1 + \epsilon]\), where \(\epsilon\) is a hyperparameter.<br></div><div>\[L^\text{CLIP} (\theta) = \mathbb{E}_t [ \min( r_t(\theta) \hat{A}_{t}, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_{t})]\]The objective function of PPO takes the minimum one between the original value and the clipped version and therefore we lose the motivation for increasing the policy update to extremes for better rewards.<br><br>When applying PPO on a network architecture with shared parameters for both policy (actor) and value (critic) functions, in addition to the clipped reward, the objective function is augmented with an error term on the value estimation (formula in red) and an entropy term (formula in blue) to encourage sufficient exploration.<br>\[L^\text{CLIP'} (\theta) = \mathbb{E}_t [ L^\text{CLIP} (\theta) - \color{red}{c_1 (V_\theta(s_t) - V_t^\text{target})^2} + \color{blue}{c_2 H(s_t, \pi_\theta(.))} ]\]</div>",Proximal Policy Optimization Algorithms,https://arxiv.org/abs/1707.06347,"<img src=""recap.jpg"">",
